---
title: "GLM VI+\nInteraction effects"
author:   "Caspar J. van Lissa"
date:     "`r format(Sys.Date(), '%d %b %Y')`"
format: revealjs
#server: shiny
---

```{r}
library(kableExtra)
require(gridExtra)
library(tidySEM)
library(scales)
library(ggplot2)
library(eulerr)
source("functions.r")
options(knitr.kable.NA = '')

invdat <- matrix(c(1, .3, .24,
                   .3, 1, .5,
                   .24, .5, 1), nrow = 3, ncol = 3, byrow = TRUE)
set.seed(76)
invdat <- data.frame(mvtnorm::rmvnorm(60, sigma = invdat))

names(invdat) <- c("Work_hours", "Gender_role", "Involvement")

invdat$Work_hours <- rescale(invdat$Work_hours, to = c(0,40))
invdat$Gender_role <- rescale(invdat$Gender_role, to = c(1,7))
invdat$Involvement <- rescale(invdat$Involvement, to = c(0,50))

m_both <- lm(Involvement ~ Work_hours + Gender_role, invdat)

m_work <- lm(Involvement ~ Work_hours, invdat)
m_gender <- lm(Involvement ~ Gender_role, invdat)
m_both <- lm(Involvement ~ Gender_role+Work_hours, invdat)
```

# Recap regression

## Regression model

$Y_i = a + b*X_i +e_{i}$

Symbol        | Interpretation
-------------- | -----------------------------------------------------
$Y_i$          | Individual i's score on dependent variable Y
$a$            | Coefficient, intercept of the regression line
$b$            | Coefficient, slope of the regression line
$X_i$          | Individual i's score on independent variable X
$e_i$   | Individual i's prediction error

## Regression line

**Predicted value (describes regression line)**

$\hat{Y}_i = a + b*X_i$, and $Y_i = \hat{Y}_i +\epsilon_{i}$

Symbol        | Interpretation
-------------- | -----------------------------------------------------
$\hat{Y}_i$    | Individual i's **predicted** score on dependent variable Y
$a$            | Coefficient, intercept of the regression line
$b$            | Coefficient, slope of the regression line
$X_i$          | Individual i's score on independent variable X


## The road so far

* $Y_i = a + bX$: Bivariate linear regression
* $Y_i = a + bX$ where $X$ is a dummy variable: comparing two groups, aka independent samples t-test
* $Y_i = a + b_1X_1 + \ldots + b_kX_k$ where $X_{1 \ldots k}$ are dummy variables: comparing multiple groups, aka ANOVA
* $Y_i = a + b_1X_1 + \ldots + b_kX_k$ where $X_{1 \ldots k}$ are continuous or dummy variables: multiple regression

Last thing we did is extend the linear model with building blocks that look like $+bX$

## Introducing: interaction

> **Interaction:** The effect of one predictor depends on the level of another predictor.

To represent this, we add a *special* building block to our regression equation:

$Y = a + b_1X_1+ b_2X_2$<font color="blue">$+ b_3(X_1*X_2)$</font>

## When to use interaction? {.smaller}

In NL, women still take on the brunt of childrearing responsibilities (parental involvement). You hypothesize that progressive gender roles will predict greater involvement for men.

* Interaction between gender roles and sex
* Continuous and dummy

Personality dimension "agreeableness" positively predicts number of friends, but only when combined with a high level of "extraversion".

* Include an interaction effect between agreeableness and extraversion
* Both are continuous variables

Treatment guidelines for heart failure are based mostly on research in men. There's recent debates that commonly prescribed drugs affect recovery in men and women differently.

* Interaction between treatment (drug vs placebo) and sex (male vs female)
* Both are dummy variables

## How to include interaction

$Y = a + b_1X_1+ b_2X_2$<font color="blue">$+ b_3(X_1*X_2)$</font>

* Calculate a new variable that is the product of the two interacting variables
* Add it to the regression model, **along with the two original variables**

::: {layout-ncol=2}

![](images/y_on_x1x2int1.png)

![](images/y_on_x1x2int2.png)

:::

# Continuous and binary

## Binary predictor

There is a difference in Parental Involvement between males (0) and females (1)

$Y_i = a + b*X_i + \epsilon_i$

This regression will give us:

* The mean level of involvement for males, $a$
* The difference in mean level of involvement between males and females, $b$
* We can calculate the mean involvement of females: $a + b$

## Binary and Continuous Predictor

Aside from a sex difference $X_1$, there is an effect of gender role attitudes, $X_2$:

$Y_i = a + b_1*X_{1i} + b_2 * X_{2i}+\epsilon_{i}$

* $a$: Mean level of involvement for males who score 0 on gender role
* $b_1$: Difference in mean level of involvement between males and females, $b$
* $b_2$: Increase in involvement associated with a 1-point increase in gender roles

## Distinct intercepts

$Y_i = a + b_1*X_{1i} + b_2 * X_{2i}+\epsilon_{i}$

Here, males and females have separate intercepts:

```{r, fig.width=4.5, fig.asp=3/4, echo=FALSE, warning=FALSE, message=FALSE}
library(scales)
set.seed(32)
n <- 178
Geslacht <- rbinom(n, 1, .5)

Genderrole <- 2*Geslacht+rnorm(n)
Genderrole <- round(scales::rescale(Genderrole, to = c(1,7)))

Involvement <- 5 + 15*Geslacht + 4*Genderrole + rnorm(n) 

dichmodel <- lm(Involvement ~ Geslacht + Genderrole)

ggplot(data.frame(Involvement, Geslacht = factor(Geslacht), Genderrole), aes(x=Genderrole, y=Involvement, colour = Geslacht))+
  geom_point()+
  geom_abline(intercept = dichmodel$coefficients[1]+dichmodel$coefficients[2], slope = dichmodel$coefficients[3], colour = "red")+
  geom_abline(intercept = dichmodel$coefficients[1], slope = dichmodel$coefficients[3], colour = "blue")+
  theme_bw()+
  scale_colour_manual(values = c("blue", "red"))+theme(legend.position = "none")

```

## Distinct regression lines

But what if we not only want to estimate distinct intercepts, but also distinct slopes for men and women?


```{r, fig.width=4.5, fig.asp=3/4, echo=FALSE, warning=FALSE, message=FALSE}
#0 man 1 vrouw
Involvement2 <- 5 + 30*Geslacht + 4*Genderrole + -3.8*Geslacht*Genderrole + rnorm(n) 

involvement2model <- lm(Involvement2 ~ Geslacht * Genderrole)

ggplot(data.frame(Involvement = Involvement2, Geslacht = factor(Geslacht), Genderrole), aes(x=Genderrole, y=Involvement, colour = Geslacht))+
  geom_point()+
  geom_abline(intercept = involvement2model$coefficients[1]+involvement2model$coefficients[2], slope = involvement2model$coefficients[3]+involvement2model$coefficients[4], colour = "red")+
  geom_abline(intercept = involvement2model$coefficients[1], slope = involvement2model$coefficients[3], colour = "blue")+
  theme_bw()+
  scale_colour_manual(values = c("blue", "red"))+theme(legend.position = "none")

```


## Interaction effect {.smaller}

For one binary predictor (male = 0, female = 1) and gender roles:

$\hat{Y}_i = a + b_1*X_{1i} + b_2 * X_{2i}+ b_3 * (X_{1i} * X_{2i})$

Symbol        | Interpretation
-------------- | ---------------------------------------------------------------
$\hat{Y}_i$    | Individual predicted value for Y (involvement)
$a$            | Expected value for men who score 0 on gender role
$b_1$          | Mean difference between men and women who score 0 on gender role
$b_2$          | Effect of gender role for men
$b_3$          | Difference in the effect of gender role between men and women


## Complete the formula {.smaller}

$\hat{Y}_i = a + b_1*X_{1i} + b_2 * X_{2i}+ b_3 * (X_{1i} * X_{2i})$

**Complete for men:**

* $$
  \begin{aligned}
  \hat{Y}_i = &a + b_1*0 + b_2 * X_{2i}+ b_3 * (0 * X_{2i}) = \\
              &a + b_2 * X_{2i}
  \end{aligned}
  $$
* So the regression line for men is $a + b_2*X_{2i}$

**Complete for women:**

* $$
  \begin{aligned}
  \hat{Y}_i = &a + b_1*1 + b_2 * X_{2i}+ b_3 * (1 * X_{2i}) = \\
   &a + b_1 + b_2 * X_{2i} + b_3 * X_{2i}
   \end{aligned}
   $$
* So the regression line for women is $(a + b_1) + (b_2+ b_3) * X_{2i}$
* An extra "bump" on top of the intercept and slope

## Examples

Which parameters are non-zero?

$\hat{Y}_i = a + b_1X_{1i} + b_2X_{2i}+b3(X_{1i}X_{2i})$

![](images/Interactie.png)

# Simple Effects

## Simple effects

If the interaction is significant, we might ask:

> What is the effect of X1 on Y for each level of G?

## For categorical moderator

Easy trick to obtain the effect for each category:

* Create k dummies for a categorical variable with k categories (instead of the usual k-1) 
* Compute interaction term with each dummy
* Specify regression with all these interaction effects, and without the main effect of the continuous variable

## Formulas {.smaller}

Standard model: 

$\hat{Y}' = b_0 + b_1 X1 + b_2 D_2 +b_3 D_3 + b_4 (D_2 X_1) + b_5 (D_3 X_1)$

* Main effect of the continuous predictor and k-1 dummies
* Interaction effect with k-1 dummies

$\hat{Y}' = b_0 + b_1 D_2 +b_2 D_3 + b_3 (D_1 X_1) + b_4 (D_2 X_1) + b_5 (D_3 X_1)$

* Main effect of k-1 dummies
* NO main effect of $X_1$
* k interaction terms (so the effect of $X_1$ is split across all k categories)

## Filling in Formulas {.smaller}

Effect of $X_1$ for group 1 ($D_2 = 0, D_3 = 0$): 

$\hat{Y}' = b_0 + b_1 X1 + b_2 *0 +b_3 *0 + b_4 (0* X_1) + b_5 (0* X_1) = b_0 + b_1 * X1$

Effect of $X_1$ for group 2 ($D_2 = 1, D_3 = 0$): 

$\hat{Y}' = b_0 + b_1 X1 + b_2*1 +b_3 *0 + b_4 (1* X_1) + b_5 (0* X_1) = (b_0 + b_2) + (b_1 + b_4) X_1$


Using the model with all interaction effects:

Effect of $X_1$ for group 1 ($D_2 = 0, D_3 = 0$): 

$\hat{Y}' = b_0 + b_1 *0 +b_2 *0 + b_3 (1* X_1) + b_4 (0* X_1) + b_5 (0* X_1) = b_0 + b_3 * X_1$

Effect of $X_1$ for group 2 ($D_2 = 1, D_3 = 0$): 

$\hat{Y}' = b_0 + b_1 *1 +b_2 *0 + b_3 (0* X_1) + b_4 (1* X_1) + b_5 (0* X_1) = (b_0 + b_1) + b_4 X_1$


# Two continuous predictors

## Difference with previous example

* An interaction between one binary and one continuous predictor results in **two regression lines**
    + One for each unique value of the binary predictor
* An interaction betweeb two continuous predictors also gives us a unique regression line for every value of each predictor
    + But both predictors can take on infinite unique values

## Binary vs continuous interaction

```{r, fig.width=4.5, fig.asp=3/4, echo=FALSE, warning=FALSE, message=FALSE}
library(scales)
require(gridExtra)
set.seed(32)
x1 <- rnorm(100)
x1 <- rescale(x1, to = c(0,40))

x2 <- rnorm(100)
x2 <- round(rescale(x2, to = c(1,7)))

y <- 2 + .10*x1 + .3*x2 + .30*x1*x2 + rnorm(100) 
y <- rescale(y, to = c(1,7))
invdat <- data.frame(Involvement=x1, Warmth=x2, Adjustment = y)
rm(x1, x2, y)
invdat_cent <- invdat
invdat_cent[,c(1,2)]<-lapply(invdat_cent[,c(1,2)], scale, scale = FALSE)
invmodel_c <- lm(Adjustment ~ Involvement * Warmth, invdat_cent)

werkplot3 <- ggplot(invdat_cent, aes(x=Involvement, y=Adjustment))+
  geom_point()+
  geom_hline(yintercept = 0)+geom_vline(xintercept = 0)+
  theme_bw()+
  scale_x_continuous(limits = c(-20,20), breaks=seq(-20,20, by = 5))+
  scale_y_continuous(limits = c(0,7), breaks=seq(0, 7, by = .5))+
  geom_abline(intercept = invmodel_c$coefficients[1], slope = invmodel_c$coefficients[2], colour = "red", size = 1)+
  geom_abline(intercept = invmodel_c$coefficients[1], slope = invmodel_c$coefficients[2]+1*invmodel_c$coefficients[4], colour = "grey60", size = 1)+
  geom_abline(intercept = invmodel_c$coefficients[1], slope = invmodel_c$coefficients[2]+3*invmodel_c$coefficients[4], colour = "grey50", size = 1)+
  geom_abline(intercept = invmodel_c$coefficients[1], slope = invmodel_c$coefficients[2]+5*invmodel_c$coefficients[4], colour = "grey40", size = 1)+
  geom_abline(intercept = invmodel_c$coefficients[1], slope = invmodel_c$coefficients[2]-1*invmodel_c$coefficients[4], colour = "grey70", size = 1)+
  geom_abline(intercept = invmodel_c$coefficients[1], slope = invmodel_c$coefficients[2]-3*invmodel_c$coefficients[4], colour = "grey80", size = 1)+
  geom_abline(intercept = invmodel_c$coefficients[1], slope = invmodel_c$coefficients[2]+7*invmodel_c$coefficients[4], colour = "grey30", size = 1)

genderplot3 <- ggplot(invdat_cent, aes(x=Warmth, y=Adjustment))+
  geom_point()+
  geom_hline(yintercept = 0)+geom_vline(xintercept = 0)+
  theme_bw()+
  scale_x_continuous(limits = c(-4,4), breaks=seq(-4,4, by = 1))+
  scale_y_continuous(limits = c(0,7), breaks=seq(0, 7, by = .5))+
  geom_abline(intercept = invmodel_c$coefficients[1], slope = invmodel_c$coefficients[3], colour = "red", size = 1)+
  geom_abline(intercept = invmodel_c$coefficients[1], slope = invmodel_c$coefficients[3]+8*invmodel_c$coefficients[4], colour = "grey60", size = 1)+
  geom_abline(intercept = invmodel_c$coefficients[1], slope = invmodel_c$coefficients[3]+16*invmodel_c$coefficients[4], colour = "grey50", size = 1)+
  geom_abline(intercept = invmodel_c$coefficients[1], slope = invmodel_c$coefficients[3]+24*invmodel_c$coefficients[4], colour = "grey40", size = 1)+
  geom_abline(intercept = invmodel_c$coefficients[1], slope = invmodel_c$coefficients[3]-8*invmodel_c$coefficients[4], colour = "grey70", size = 1)+
  geom_abline(intercept = invmodel_c$coefficients[1], slope = invmodel_c$coefficients[3]-16*invmodel_c$coefficients[4], colour = "grey80", size = 1)+
  geom_abline(intercept = invmodel_c$coefficients[1], slope = invmodel_c$coefficients[3]-24*invmodel_c$coefficients[4], colour = "grey90", size = 1)

grid.arrange(werkplot3, genderplot3, ncol=2)

```

## Multiple regression

```{r, echo = FALSE, message=FALSE}
library(plotly)
library(reshape2)
library(scales)
require(gridExtra)
set.seed(32)
x1 <- rnorm(100)
x1 <- rescale(x1, to = c(0,40))

x2 <- rnorm(100)
x2 <- round(rescale(x2, to = c(1,7)))

y <- 20 - .25*x1 + 4*x2 + rnorm(100) 

invdat <- data.frame(Work_hours=x1, Genderrole=x2, Involvement = y)
rm(x1, x2, y)

invmodel <- lm(Involvement ~ Work_hours + Genderrole, invdat)

graph_reso <- .5

#Setup Axis
axis_x <- seq(min(invdat$Work_hours), max(invdat$Work_hours), by = graph_reso)
axis_y <- seq(min(invdat$Genderrole), max(invdat$Genderrole), by = graph_reso)

#Sample points
inv_lm_surface <- expand.grid(Work_hours = axis_x, Genderrole = axis_y, KEEP.OUT.ATTRS = F)
inv_lm_surface$Involvement <- predict.lm(invmodel, newdata = inv_lm_surface)
inv_lm_surface <- acast(inv_lm_surface, Genderrole ~ Work_hours, value.var = "Involvement") #y ~ x

invplot <- plot_ly() %>%
  add_surface(x = axis_x, 
                  y = axis_y, 
                  z = inv_lm_surface, 
                  type = "surface") %>%#, 
                  #opacity = 1,
                  #colors = c('#d1d1d1','#000000')) %>%
  add_trace(x = invdat$Work_hours, 
            y = invdat$Genderrole,
            z = invdat$Involvement, 
            type = "scatter3d", 
            mode = "markers",
            marker = list(color = "red"),
            #color = coh$dance,
            #colors = c("gray70", '#6d98f3'),
            opacity = 1) %>%
  layout(title = "Multiple regression demo",
         scene = list(xaxis = list(title = 'Work hours', range = c(0,40)),# ticktype = "array", tickvals = ticks),
                      yaxis = list(title = 'Gender role', range = c(1,7)),# ticktype = "array", tickvals = ticks),
                      zaxis = list(title = 'Involvement', range = c(0,50)),# ticktype = "array", tickvals = ticks),
                      camera = list(eye = list(x = 2, y = -2, z = 1.25), zoom = 5),
                      showlegend = FALSE))
invplot
```

## Multiple regression with interaction

```{r, echo = FALSE, message=FALSE}
set.seed(32)
x1 <- rnorm(100)
x1 <- rescale(x1, to = c(0,40))

x2 <- rnorm(100)
x2 <- round(rescale(x2, to = c(1,7)))

y <- 2 + .10*x1 + -1*x2 + 2*x1*x2 + rnorm(100) 
y <- rescale(y, to = c(1,7))
invdat <- data.frame(Involvement=x1, Warmth=x2, Adjustment = y)
rm(x1, x2, y)
invdat[,c(1,2)]<-lapply(invdat[,c(1,2)], function(x){
  c(scale(x, scale = FALSE))})

invmodel <- lm(Adjustment ~ Involvement * Warmth, invdat)

graph_reso <- .5

#Setup Axis
axis_x <- c(seq(min(invdat$Involvement), max(invdat$Involvement), by = graph_reso))
axis_y <- c(seq(min(invdat$Warmth), max(invdat$Warmth), by = graph_reso))

#Sample points
inv_lm_surface <- expand.grid(Involvement = axis_x, Warmth = axis_y, KEEP.OUT.ATTRS = F)
inv_lm_surface$Adjustment <- predict.lm(invmodel, newdata = inv_lm_surface)
inv_lm_surface <- acast(inv_lm_surface, Warmth ~ Involvement, value.var = "Adjustment") #y ~ x

invplot <- plot_ly() %>%
  add_surface(x = axis_x, 
                  y = axis_y, 
                  z = inv_lm_surface, 
                  type = "surface") %>%#, 
                  #opacity = 1,
                  #colors = c('#d1d1d1','#000000')) %>%
  add_trace(x = invdat$Involvement, 
            y = invdat$Warmth,
            z = invdat$Adjustment, 
            type = "scatter3d", 
            mode = "markers",
            marker = list(color = "red"),
            #color = coh$dance,
            #colors = c("gray70", '#6d98f3'),
            opacity = 1) %>%
  layout(title = "Multiple regression demo",
         scene = list(xaxis = list(title = 'Involvement', range = c(-25,25)),# ticktype = "array", tickvals = ticks),
                      yaxis = list(title = 'Warmth', range = c(-4,4)),# ticktype = "array", tickvals = ticks),
                      zaxis = list(title = 'Adjustment', range = c(0,7)),# ticktype = "array", tickvals = ticks),
                      camera = list(eye = list(x = 2, y = -2, z = 1.25), zoom = 5),
                      showlegend = FALSE))
invplot
```


## Complete the formula {.smaller}

$\hat{Y}_i = a + b_1*X_{1i} + b_2 * X_{2i}+ b_3 * (X_{1i} * X_{2i})$

* Y is involvement, X1 is gender roles, X2 is work
* Imagine we have found these coefficients:

$\hat{Y}_i = 12.50 + 1.50*X_{1i} - .20 * X_{2i} + 0.07 * (X_{1i} * X_{2i})$

What's the effect of gender roles for someone who works 40 hours?

$\hat{Y}_i = 12.50 + 1.50*X_{1i} - .20 * 40 + 0.07 * (40 * X_{1i})$

$\hat{Y}_i = (12.50- .20 * 40) + (1.50+0.07 * 40)*X_{1i} = 4.5 + 4.3*X_{1i}$

## Complete the formula {.smaller}

$\hat{Y}_i = a + b_1*X_{1i} + b_2 * X_{2i}+ b_3 * (X_{1i} * X_{2i})$

Imagine we have found these coefficients:

$\hat{Y}_i = 12.50 + 1.50*X_{1i} - .20 * X_{2i} + 0.07 * (X_{1i} * X_{2i})$

What's the effect of work hours for someone who scores 0 on gender roles?

$\hat{Y}_i = 12.50 + 1.50*0 - .20 * X_{2i} + 0.07 * (0 * X_{2i})$

$\hat{Y}_i = (12.50 + 1.50*0) - (.20 + 0.07 * 0) * X_{2i} = 12.50 - .20 * X_{2i}$

## Centering {.smaller}

As the effect of X1 is now dependent on the value of X2, and vice versa, it's essential to **center the variables**

Not centering...

* Makes coefficients hard to interpret
* Introduces (artificial) multicolinearity

When you center, the interpretation is:

* a: Expected value of Y for people who score average on all predictors
* b1: Slope of predictor 1 for people who score average on predictor 2
* b2: Slope of predictor 2 for people who score average on predictor 1


# Simple Slopes

## Simple Slopes

If the interaction is significant, we might ask:

> What is the effect of X1 on Y for different levels of X2?

With a categorical moderator, we obtained the effect of X1 for each discrete level of X2

With a continuous moderator, we need to pick specific values of X2

## Which one is the moderator? {.smaller}

Mathematically, there is no difference between saying:

* The effect of X1 depends on the value of X2
* The effect of X2 depends on the value of X1
* Because $X1*X2$ is the same as $X2*X1$

Theory makes the difference! You decide which variable moderates the effect of the other

## Use centering

Remember that multiple regression gives you the effect of each predictor while controlling all other predictors at zero

* You can change the zero-value
* You've done this before: by centering variables you make the zero value equal to "the mean" of that variable
* With centered predictors centered at the mean, regression with interaction gives us the effect of X1 for people with an average score on X2

## Center to high and low values

To get the effect of X1 for people who score high on X2, just center X2 to a high value! 

* Center X2 at +1 SD
* Center X2 at -1 SD

Recompute the regression for each of these re-centered variables

## Simple slopes steps

1. Center the interacting predictors at their mean value
1. Compute the interaction term
1. Determine whether the interaction is significant
1. Center X2 at +1SD
1. Re-compute the interaction term
1. Note the effect of X1 for this level of X2
1. Center X2 at -1SD
1. Re-compute the interaction term
1. Note the effect of X1 for this level of X2

## Important

* To center at +1SD you have to **subtract** 1 SD from the centered variable
* To center at -1SD you have to **add** 1 SD from the centered variable

```{r}
library(ggplot2)
ggplot(data = data.frame(x = c(-5, 5)), aes(x)) +
  stat_function(fun = dnorm, args = list(mean = 0, sd = 1), color = "black") + ylab("") +
  stat_function(fun = dnorm, args = list(mean = -1, sd = 1), color = "red") + ylab("") +
  stat_function(fun = dnorm, args = list(mean = +1, sd = 1), color = "blue") + ylab("") +
  geom_label(x = -1, y = dnorm(0, 0, 1), vjust = 0, hjust = 1,label = "X - M - 1SD)", fill = "red", alpha = .2) +
  geom_label(x = 1, y = dnorm(0, 0, 1), vjust = 0, hjust = 0,label = "X - M + 1SD)", fill = "blue", alpha = .2) +
  geom_vline(xintercept = 0, linetype = 2)+
  scale_y_continuous(expand = c(.004, .04), breaks = NULL)+
  theme_bw()
    
```

## Important 2

After changing how a variable is centered, you have to re-compute the interaction term

* Make sure you always use the correctly centered variable and its corresponding interaction term
* Using syntax helps prevent mistakes
