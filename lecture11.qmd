---
title: "Lecture 11 - GLM VII\nInteraction effects"
author:   "Caspar J. van Lissa"
date:     "`r format(Sys.Date(), '%d %b %Y')`"
format: revealjs
#server: shiny
---

```{r}
library(kableExtra)
require(gridExtra)
library(tidySEM)
library(scales)
library(eulerr)
source("functions.r")
options(knitr.kable.NA = '')

invdat <- matrix(c(1, .3, .24,
                   .3, 1, .5,
                   .24, .5, 1), nrow = 3, ncol = 3, byrow = TRUE)
set.seed(76)
invdat <- data.frame(mvtnorm::rmvnorm(60, sigma = invdat))

names(invdat) <- c("Work_hours", "Gender_role", "Involvement")

invdat$Work_hours <- rescale(invdat$Work_hours, to = c(0,40))
invdat$Gender_role <- rescale(invdat$Gender_role, to = c(1,7))
invdat$Involvement <- rescale(invdat$Involvement, to = c(0,50))

m_both <- lm(Involvement ~ Work_hours + Gender_role, invdat)

m_work <- lm(Involvement ~ Work_hours, invdat)
m_gender <- lm(Involvement ~ Gender_role, invdat)
m_both <- lm(Involvement ~ Gender_role+Work_hours, invdat)
```

# Recap regression

## Regression model

$Y_i = a + b*X_i +e_{i}$

Symbol        | Interpretation
-------------- | -----------------------------------------------------
$Y_i$          | Individual i's score on dependent variable Y
$a$            | Coefficient, intercept of the regression line
$b$            | Coefficient, slope of the regression line
$X_i$          | Individual i's score on independent variable X
$e_i$   | Individual i's prediction error

## Regression line

**Predicted value (describes regression line)**

$\hat{Y}_i = a + b*X_i$, and $Y_i = \hat{Y}_i +\epsilon_{i}$

Symbol        | Interpretation
-------------- | -----------------------------------------------------
$\hat{Y}_i$    | Individual i's **predicted** score on dependent variable Y
$a$            | Coefficient, intercept of the regression line
$b$            | Coefficient, slope of the regression line
$X_i$          | Individual i's score on independent variable X


## The road so far

* $Y_i = a + bX$: Bivariate linear regression
* $Y_i = a + bX$ where $X$ is a dummy variable: comparing two groups, aka independent samples t-test
* $Y_i = a + b_1X_1 + \ldots + b_kX_k$ where $X_{1 \ldots k}$ are dummy variables: comparing multiple groups, aka ANOVA
* $Y_i = a + b_1X_1 + \ldots + b_kX_k$ where $X_{1 \ldots k}$ are continuous or dummy variables: multiple regression

Last thing we did is extend the linear model with building blocks that look like $+bX$

## Introducing: interaction

> **Interaction:** The effect of one predictor depends on the level of another predictor.

To represent this, we add a *special* building block to our regression equation:

$Y = a + b_1X_1+ b_2X_2$<font color="blue">$+ b_3(X_1*X_2)$</font>

## When to use interaction? {.smaller}

In NL, women still take on the brunt of childrearing responsibilities (parental involvement). You hypothesize that progressive gender roles will predict greater involvement for men.

* Interaction between gender roles and sex
* Continuous and dummy

Personality dimension "agreeableness" positively predicts number of friends, but only when combined with a high level of "extraversion".

* Include an interaction effect between agreeableness and extraversion
* Both are continuous variables

Treatment guidelines for heart failure are based mostly on research in men. There's recent debates that commonly prescribed drugs affect recovery in men and women differently.

* Interaction between treatment (drug vs placebo) and sex (male vs female)
* Both are dummy variables

## How to include interaction

$Y = a + b_1X_1+ b_2X_2$<font color="blue">$+ b_3(X_1*X_2)$</font>

* Calculate a new variable that is the product of the two interacting variables
* Add it to the regression model, along with the two original variables

::: {layout-ncol=2}

![](images/y_on_x1x2int1.png)

![](images/y_on_x1x2int2.png)

:::

# Continuous and binary

## Binary predictor

There is a difference in Parental Involvement between males (0) and females (1)

$Y_i = a + b*X_i + \epsilon_i$

This regression will give us:

* The mean level of involvement for males, $a$
* The difference in mean level of involvement between males and females, $b$
* We can calculate the mean involvement of females: $a + b$

## Binary and Continuous Predictor

Aside from a sex difference $X_1$, there is an effect of gender role attitudes, $X_2$:

$Y_i = a + b_1*X_{1i} + b_2 * X_{2i}+\epsilon_{i}$

* $a$: Mean level of involvement for males who score 0 on gender role
* $b_1$: Difference in mean level of involvement between males and females, $b$
* $b_2$: Increase in involvement associated with a 1-point increase in gender roles

## Distinct intercepts

$Y_i = a + b_1*X_{1i} + b_2 * X_{2i}+\epsilon_{i}$

Here, males and females have separate intercepts:

```{r, fig.width=4.5, fig.asp=3/4, echo=FALSE, warning=FALSE, message=FALSE}
library(scales)
set.seed(32)
n <- 178
Geslacht <- rbinom(n, 1, .5)

Genderrole <- 2*Geslacht+rnorm(n)
Genderrole <- round(scales::rescale(Genderrole, to = c(1,7)))

Involvement <- 5 + 15*Geslacht + 4*Genderrole + rnorm(n) 

dichmodel <- lm(Involvement ~ Geslacht + Genderrole)

ggplot(data.frame(Involvement, Geslacht = factor(Geslacht), Genderrole), aes(x=Genderrole, y=Involvement, colour = Geslacht))+
  geom_point()+
  geom_abline(intercept = dichmodel$coefficients[1]+dichmodel$coefficients[2], slope = dichmodel$coefficients[3], colour = "red")+
  geom_abline(intercept = dichmodel$coefficients[1], slope = dichmodel$coefficients[3], colour = "blue")+
  theme_bw()+
  scale_colour_manual(values = c("blue", "red"))+theme(legend.position = "none")

```

## Distinct regression lines

But what if we not only want to estimate distinct intercepts, but also distinct slopes for men and women?


```{r, fig.width=4.5, fig.asp=3/4, echo=FALSE, warning=FALSE, message=FALSE}
#0 man 1 vrouw
Involvement2 <- 5 + 30*Geslacht + 4*Genderrole + -3.8*Geslacht*Genderrole + rnorm(n) 

involvement2model <- lm(Involvement2 ~ Geslacht * Genderrole)

ggplot(data.frame(Involvement = Involvement2, Geslacht = factor(Geslacht), Genderrole), aes(x=Genderrole, y=Involvement, colour = Geslacht))+
  geom_point()+
  geom_abline(intercept = involvement2model$coefficients[1]+involvement2model$coefficients[2], slope = involvement2model$coefficients[3]+involvement2model$coefficients[4], colour = "red")+
  geom_abline(intercept = involvement2model$coefficients[1], slope = involvement2model$coefficients[3], colour = "blue")+
  theme_bw()+
  scale_colour_manual(values = c("blue", "red"))+theme(legend.position = "none")

```


## Interaction effect

For one binary predictor (male = 0, female = 1) and gender roles:

$\hat{Y}_i = a + b_1*X_{1i} + b_2 * X_{2i}+ b_3 * (X_{1i} * X_{2i})$

Symbol        | Interpretation
-------------- | ---------------------------------------------------------------
$\hat{Y}_i$    | Individual predicted value for Y (involvement)
$a$            | Expected value for men who score 0 on gender role
$b_1$          | Mean difference between men and women who score 0 on gender role
$b_2$          | Effect of gender role for men
$b_3$          | Difference in the effect of gender role between men and women


## Complete the formula

$\hat{Y}_i = a + b_1*X_{1i} + b_2 * X_{2i}+ b_3 * (X_{1i} * X_{2i})$

**Complete for men:**

* $$
  \begin{aligned}
  \hat{Y}_i = &a + b_1*0 + b_2 * X_{2i}+ b_3 * (0 * X_{2i}) = \\
              &a + b_2 * X_{2i}
  \end{aligned}
  $$
* So the regression line for men is $a + b_2*X_{2i}$

**Complete for women:**

* $$
  \begin{aligned}
  \hat{Y}_i = &a + b_1*1 + b_2 * X_{2i}+ b_3 * (1 * X_{2i}) = \\
   &a + b_1 + b_2 * X_{2i} + b_3 * X_{2i}
   \end{aligned}
   $$
* So the regression line for women is $(a + b_1) + (b_2+ b_3) * X_{2i}$
* An extra "bump" on top of the intercept and slope

## Examples

Which parameters are non-zero?

$\hat{Y}_i = a + b_1X_{1i} + b_2X_{2i}+b3(X_{1i}X_{2i})$

![](images/Interactie.png)

# Two continuous predictors

## Difference with previous example

* An interaction between one binary and one continuous predictor results in **two regression lines**
    + One for each unique value of the binary predictor
* An interaction betweeb two continuous predictors also gives us a unique regression line for every value of each predictor
    + But both predictors can take on infinite unique values

## Binary vs continuous interaction

```{r, fig.width=4.5, fig.asp=3/4, echo=FALSE, warning=FALSE, message=FALSE}
library(scales)
require(gridExtra)
set.seed(32)
x1 <- rnorm(100)
x1 <- rescale(x1, to = c(0,40))

x2 <- rnorm(100)
x2 <- round(rescale(x2, to = c(1,7)))

y <- 2 + .10*x1 + .3*x2 + .30*x1*x2 + rnorm(100) 
y <- rescale(y, to = c(1,7))
invdat <- data.frame(Involvement=x1, Warmth=x2, Adjustment = y)
rm(x1, x2, y)
invdat_cent <- invdat
invdat_cent[,c(1,2)]<-lapply(invdat_cent[,c(1,2)], scale, scale = FALSE)
invmodel_c <- lm(Adjustment ~ Involvement * Warmth, invdat_cent)

werkplot3 <- ggplot(invdat_cent, aes(x=Involvement, y=Adjustment))+
  geom_point()+
  geom_hline(yintercept = 0)+geom_vline(xintercept = 0)+
  theme_bw()+
  scale_x_continuous(limits = c(-20,20), breaks=seq(-20,20, by = 5))+
  scale_y_continuous(limits = c(0,7), breaks=seq(0, 7, by = .5))+
  geom_abline(intercept = invmodel_c$coefficients[1], slope = invmodel_c$coefficients[2], colour = "red", size = 1)+
  geom_abline(intercept = invmodel_c$coefficients[1], slope = invmodel_c$coefficients[2]+1*invmodel_c$coefficients[4], colour = "grey60", size = 1)+
  geom_abline(intercept = invmodel_c$coefficients[1], slope = invmodel_c$coefficients[2]+3*invmodel_c$coefficients[4], colour = "grey50", size = 1)+
  geom_abline(intercept = invmodel_c$coefficients[1], slope = invmodel_c$coefficients[2]+5*invmodel_c$coefficients[4], colour = "grey40", size = 1)+
  geom_abline(intercept = invmodel_c$coefficients[1], slope = invmodel_c$coefficients[2]-1*invmodel_c$coefficients[4], colour = "grey70", size = 1)+
  geom_abline(intercept = invmodel_c$coefficients[1], slope = invmodel_c$coefficients[2]-3*invmodel_c$coefficients[4], colour = "grey80", size = 1)+
  geom_abline(intercept = invmodel_c$coefficients[1], slope = invmodel_c$coefficients[2]+7*invmodel_c$coefficients[4], colour = "grey30", size = 1)

genderplot3 <- ggplot(invdat_cent, aes(x=Warmth, y=Adjustment))+
  geom_point()+
  geom_hline(yintercept = 0)+geom_vline(xintercept = 0)+
  theme_bw()+
  scale_x_continuous(limits = c(-4,4), breaks=seq(-4,4, by = 1))+
  scale_y_continuous(limits = c(0,7), breaks=seq(0, 7, by = .5))+
  geom_abline(intercept = invmodel_c$coefficients[1], slope = invmodel_c$coefficients[3], colour = "red", size = 1)+
  geom_abline(intercept = invmodel_c$coefficients[1], slope = invmodel_c$coefficients[3]+8*invmodel_c$coefficients[4], colour = "grey60", size = 1)+
  geom_abline(intercept = invmodel_c$coefficients[1], slope = invmodel_c$coefficients[3]+16*invmodel_c$coefficients[4], colour = "grey50", size = 1)+
  geom_abline(intercept = invmodel_c$coefficients[1], slope = invmodel_c$coefficients[3]+24*invmodel_c$coefficients[4], colour = "grey40", size = 1)+
  geom_abline(intercept = invmodel_c$coefficients[1], slope = invmodel_c$coefficients[3]-8*invmodel_c$coefficients[4], colour = "grey70", size = 1)+
  geom_abline(intercept = invmodel_c$coefficients[1], slope = invmodel_c$coefficients[3]-16*invmodel_c$coefficients[4], colour = "grey80", size = 1)+
  geom_abline(intercept = invmodel_c$coefficients[1], slope = invmodel_c$coefficients[3]-24*invmodel_c$coefficients[4], colour = "grey90", size = 1)

grid.arrange(werkplot3, genderplot3, ncol=2)

```

## Multiple regression

```{r, echo = FALSE, message=FALSE}
library(plotly)
library(reshape2)
library(scales)
require(gridExtra)
set.seed(32)
x1 <- rnorm(100)
x1 <- rescale(x1, to = c(0,40))

x2 <- rnorm(100)
x2 <- round(rescale(x2, to = c(1,7)))

y <- 20 - .25*x1 + 4*x2 + rnorm(100) 

invdat <- data.frame(Werkuren=x1, Genderrole=x2, Involvement = y)
rm(x1, x2, y)

invmodel <- lm(Involvement ~ Werkuren + Genderrole, invdat)

graph_reso <- .5

#Setup Axis
axis_x <- seq(min(invdat$Werkuren), max(invdat$Werkuren), by = graph_reso)
axis_y <- seq(min(invdat$Genderrole), max(invdat$Genderrole), by = graph_reso)

#Sample points
inv_lm_surface <- expand.grid(Werkuren = axis_x, Genderrole = axis_y, KEEP.OUT.ATTRS = F)
inv_lm_surface$Involvement <- predict.lm(invmodel, newdata = inv_lm_surface)
inv_lm_surface <- acast(inv_lm_surface, Genderrole ~ Werkuren, value.var = "Involvement") #y ~ x

invplot <- plot_ly() %>%
  add_surface(x = axis_x, 
                  y = axis_y, 
                  z = inv_lm_surface, 
                  type = "surface") %>%#, 
                  #opacity = 1,
                  #colors = c('#d1d1d1','#000000')) %>%
  add_trace(x = invdat$Werkuren, 
            y = invdat$Genderrole,
            z = invdat$Involvement, 
            type = "scatter3d", 
            mode = "markers",
            marker = list(color = "red"),
            #color = coh$dance,
            #colors = c("gray70", '#6d98f3'),
            opacity = 1) %>%
  layout(title = "Multiple regression demo",
         scene = list(xaxis = list(title = 'Werkuren', range = c(0,40)),# ticktype = "array", tickvals = ticks),
                      yaxis = list(title = 'Gender role', range = c(1,7)),# ticktype = "array", tickvals = ticks),
                      zaxis = list(title = 'Involvement', range = c(0,50)),# ticktype = "array", tickvals = ticks),
                      camera = list(eye = list(x = 2, y = -2, z = 1.25), zoom = 5),
                      showlegend = FALSE))
invplot
```

## Multiple regression with interaction

```{r, echo = FALSE, message=FALSE}
set.seed(32)
x1 <- rnorm(100)
x1 <- rescale(x1, to = c(0,40))

x2 <- rnorm(100)
x2 <- round(rescale(x2, to = c(1,7)))

y <- 2 + .10*x1 + -1*x2 + 2*x1*x2 + rnorm(100) 
y <- rescale(y, to = c(1,7))
invdat <- data.frame(Involvement=x1, Warmth=x2, Adjustment = y)
rm(x1, x2, y)
invdat[,c(1,2)]<-lapply(invdat[,c(1,2)], function(x){
  c(scale(x, scale = FALSE))})

invmodel <- lm(Adjustment ~ Involvement * Warmth, invdat)

graph_reso <- .5

#Setup Axis
axis_x <- c(seq(min(invdat$Involvement), max(invdat$Involvement), by = graph_reso))
axis_y <- c(seq(min(invdat$Warmth), max(invdat$Warmth), by = graph_reso))

#Sample points
inv_lm_surface <- expand.grid(Involvement = axis_x, Warmth = axis_y, KEEP.OUT.ATTRS = F)
inv_lm_surface$Adjustment <- predict.lm(invmodel, newdata = inv_lm_surface)
inv_lm_surface <- acast(inv_lm_surface, Warmth ~ Involvement, value.var = "Adjustment") #y ~ x

invplot <- plot_ly() %>%
  add_surface(x = axis_x, 
                  y = axis_y, 
                  z = inv_lm_surface, 
                  type = "surface") %>%#, 
                  #opacity = 1,
                  #colors = c('#d1d1d1','#000000')) %>%
  add_trace(x = invdat$Involvement, 
            y = invdat$Warmth,
            z = invdat$Adjustment, 
            type = "scatter3d", 
            mode = "markers",
            marker = list(color = "red"),
            #color = coh$dance,
            #colors = c("gray70", '#6d98f3'),
            opacity = 1) %>%
  layout(title = "Multiple regression demo",
         scene = list(xaxis = list(title = 'Involvement', range = c(-25,25)),# ticktype = "array", tickvals = ticks),
                      yaxis = list(title = 'Warmth', range = c(-4,4)),# ticktype = "array", tickvals = ticks),
                      zaxis = list(title = 'Adjustment', range = c(0,7)),# ticktype = "array", tickvals = ticks),
                      camera = list(eye = list(x = 2, y = -2, z = 1.25), zoom = 5),
                      showlegend = FALSE))
invplot
```


## Complete the formula

$\hat{Y}_i = a + b_1*X_{1i} + b_2 * X_{2i}+ b_3 * (X_{1i} * X_{2i})$

Imagine we have found these coefficients:

$\hat{\text{Involvement}}_i = 12.50 + 1.50*\text{GR}_{i} - .20 * \text{Work}_{i} + 0.07 * (\text{GR}_{i} * \text{Work}_{i})$

What's the effect of gender roles for someone who works 40 hours?

$\hat{\text{Involvement}}_i = 12.50 + 1.50*\text{GR}_{i} - .20 * 40 + 0.07 * (40 * \text{Work}_{i})$

$\hat{\text{Involvement}}_i = (12.50- .20 * 40) + (1.50+0.07 * 40)*\text{GR}_{i} = 4.5 + 4.3*\text{GR}_{i}$

## Complete the formula

$\hat{Y}_i = a + b_1*X_{1i} + b_2 * X_{2i}+ b_3 * (X_{1i} * X_{2i})$

Imagine we have found these coefficients:

$\hat{\text{Involvement}}_i = 12.50 + 1.50*\text{GR}_{i} - .20 * \text{Work}_{i} + 0.07 * (\text{GR}_{i} * \text{Work}_{i})$

What's the effect of work hours for someone who scores 0 on gender roles?

$\hat{\text{Involvement}}_i = 12.50 + 1.50*0 - .20 * \text{Work}_{i} + 0.07 * (0 * \text{Work}_{i})$

$\hat{\text{Involvement}}_i = (12.50 + 1.50*0) - (.20 + 0.07 * 0) * \text{Work}_{i} = 12.50 - .20 * \text{Work}_{i}$

## Centering

Omdat nu het effect van X1 afhangt van de waarde van X2, en andersom, is **centreren essentieel**

* Anders oninterpreteerbare coefficienten
* Ook verhoogt het (kunstmatig) de multicolineariteit

Als je centreert is de interpretatie:

* a: Gemiddelde waarde van Y voor mensen die gemiddeld scoren op alle predictoren
* b1: hellingsgetal van predictor 1, voor mensen die gemiddeld scoren op predictor 2
* b2: hellingsgetal van predictor 2, voor mensen die gemiddeld scoren op predictor 1

## Centreren

Reviews van een artikel waar ik bij betrokken ben:

> **Reply to reviewer comments by reviewer 1**  
  I am still very concerned about multicollinearity. The threshold of VIF = 10 is too big, 3 is more proper. I recommend the authors to follow the method (Zuur et al., 2010).

> *We understand that the reviewer is still concerned about multicollinearity in our
previous analysis; we have made further efforts to address this concern. Although the original manuscript showed some VIFs >3 and >10, this is because we used only uncentered predictors in the model, including for the interaction term. This leads to artificial multicollinearity between the interaction term and its constituent terms. After centering, all VIFs are <2.5*
