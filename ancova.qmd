# GLM: ANCOVA {#sec-ancova}

```{r, include = FALSE}
source("R/booktem_setup.R")
source("R/my_setup.R")
```

ANCOVA, which stands for Analysis of Covariance, is an extension of the concepts we've covered in bivariate linear regression and multiple regression.
It is essentially a multiple regression with a categorical predictor and one or more continuous predictors. What's "special" about this technique is that it is commonly used when the predictor of interest is that categorical variable, and the continuous predictor(s) are so-called "covariates": predictors that are only included to improve our estimate of the effect of the categorical predictor of interest.

You will often see this technique used to analyze data from experiments or "natural experiments", where participants self-select into a treatment group.

While ANCOVA is a useful technique, it comes with some serious pitfalls: any time control variables are used, we are making assumptions about causality. If these assumptions are incorrect, our estimates of the effect of interest will be (severely) biased.

## Covariates and Their Role

Covariates are variables that have a relationship with the dependent variable but are not the primary focus of the study. They are often referred to as control variables, as they help control for unwanted variability and improve the precision of the analysis. Examples of common covariates include age, gender, education level, or any other variables that might influence the dependent variable.

In terms of causality, it's crucial to consider the relationships between covariates, predictors, and the outcome variable. Control variables should ideally be confounders – variables that influence both the predictor of interest and the outcome. It's essential to avoid controlling for colliders, which are variables *caused by* both the predictor and the outcome. A thorough understanding of causal relationships is crucial for proper interpretation.

One reason why researchers use control variables in ANCOVA is because they reduce the residual variance in the outcome variable, which in turn increases the power to detect the effect of the predictor of interest. Another reason to use covariates is when the goal is making causal inferences, especially in quasi-experimental designs. The proper selection of covariates that enable causal inference requires careful consideration and is beyond the scope of this course.

## Good, Neutral, and Bad Controls

Covariates fall into different categories based on their relationship with the predictor of interest and the outcome. An example of a good control is a confounder: a variable that causes both the predictor and the outcome. These need to be controlled to avoid spurious relationships. An example of a neutral control is a covariate that is unrelated to the predictor but can reduce error variance in the outcome, thereby increasing statistical power. Bad controls, on the other hand, can introduce biases, such as collider bias (controlling for an outcome of predictor and outcome), case control bias (controlling for an outcome of the outcome), or overcontrol bias (controlling for a mediator of the effect of the focal predictor on the outcome).

One crucial insight is that in randomized controlled experiments, the random assignment of participants to different groups breaks the relationship between confounders and the treatment variable. This makes control variables related to the confounders unnecessary. Controlling for them could even introduce bias into the analysis.


## Calculating Adjusted Means

Adjusting for covariates involves calculating adjusted means – the means that groups would have had if they scored equally on the covariate. There are two ways to mathematically calculate the adjusted means. One way is to fill in the regression equation for the desired value of the covariate. 

The other way is to calculate the adjusted means from the group means:

$$
\bar{Y}_g^{adj} = \bar{Y}_g - b(\bar{X}_g-\bar{X})
$$

Where:

* $\bar{Y}_g^{adj}$: Adjusted mean of the outcome for group g
* $\bar{Y}_g$: Unadjusted mean of the outcome for group g
* $b$: Regression coefficient of the covariate
* $\bar{X}_g$: Group mean of covariate X
* $\bar{X}$: Overall mean of covariate X


In sum, ANCOVA is a different name for regression with a categorical predictor of interest, and continuous predictor(s) that are included to improve our estimate of the effect of the predictor of interest. ANCOVA can enhance statistical power and help make more accurate (potentiall causal) inferences. However, the careful selection of covariates and an understanding of causal relationships are paramount to its proper implementation and interpretation.

# Lecture

{{< video https://www.youtube.com/embed/MewqUBfQYok >}}


# Formative Test

A formative test helps you assess your progress in the course, and helps you address any blind spots in your understanding of the material. If you get a question wrong, you will receive a hint on how to improve your understanding of the material.

Complete the formative test ideally after you’ve seen the lecture, but before the lecture meeting in which we can discuss any topics that need more attention

```{r}
#| results = "asis",
#| echo = FALSE,
#| cache = FALSE
add_mcs("questions_ancova.csv")
```

# Tutorial

## Bivariate Regression (RECAP)

Researchers are interested in the relationship between age and depression.

They hypothesized that older people are more vulnerable to depressive thoughts than younger people.

To test their research hypothesis, they collected data in a random sample of 164 persons from the general population. Open the dataset `HADShealthyGroup.sav`.

Run a linear regression analysis using age as the independent variable and depression as the dependent variable.

Proceed as follows:

Navigate to Analyze > Regression > Linear

Select the correct dependent and independent variable.

Paste and run the syntax.
 
How much of the total variance in Depression is explained by Age? `r fitb(0.02, num = T, tol = .005)`

What can you say about the effect size? Would you say it’s a lot?

`r hide("Answer")`
To me, 2% of explained variance does not seem like a lot. There are probably better predictors of depression (i.e., predictors that explain more of the variance in depression).

`r unhide()`

True or false: The explained variance is significant at the 10% level. `r torf(FALSE)` 

`r hide("Answer")`
To conclude anything about the significance of the proportion explain variance of this model, we can look at the ANOVA table or ask SPSS to show the R2-change. This shows us that the explained variance in this model is not significant compared to an empty model (i.e. including no predictors), F(1,140) = 2.836, p = .094.
 

`r unhide()`

Write down the estimated regression line using the unstandardized coefficients.

$Y^{'} = `r fitb(1.89, num = T, tol = .01)` + `r fitb(0.02, num = T, tol = .005)`*\text{age}$

How can we interpret the constant?

`r longmcq(c(
answer = "The predicted level of depression when age would be 0.",
"The average level of depression.",
"The average age in the sample.",
"The predicted level of depression for the average age."
)[sample.int(4)])`

Consult the table with the coefficients again

True or false: We can conclude from this table that the effect of age is significant at the 10% level. `r torf(TRUE)`

`r hide("Answer")`
To conclude whether the effect of Age on Depression is significant, we look at the t-test for the estimated coefficient.

We should conclude that the effect of Age on Depression is significant when using α=.10, t(140) = 1.684, p = .094.
`r unhide()`

One of the assumptions of bivariate regression analysis is that the relationship between the independent and dependent variable is linear.

State in your own words what this assumption entails.

`r hide("Answer")`
The assumption of a linear relationship entails that the relationship between the variables can be described with a straight line.

`r unhide()`

 
How would you evaluate the assumption of linearity graphically? Do it for the data at hand.

True or false: The relationship is linear. `r torf(FALSE)`

If the assumption is not met, speculate about other possible relationships between age and depression.

`r hide("Answer")`
The scatter plot does not have the shape of a cigar, so it does not unambiguously suggest a linear relationship. Thus, you may doubt whether the relationship between age and depression is best described by a linear model.
Perhaps the relationship is quadratic. Especially persons in middle ages may be vulnerable to depressive thoughts.
Next to the plot, you find both the estimated linear trend and a non-linear trend. It seems that the quadratic curve fits better with the data. Moreover, the quadratic model explains 6% of the variance, whereas the linear model only 2%.
`r unhide()`

Run a regression analysis using Age as the independent variable and Anxiety as the dependent variable.

Summarize the results.

Include in your answer the proportion of explained variance (R-square), a description of the effect based on the estimated regression coefficients, and evaluate the significance of the effect of age on anxiety.

`r hide("Answer")`
The proportion explained variance in Anxiety by Age is .071. The explained variance in this model is not significant compared to an empty model (i.e. including no predictors), F(1,140) = 0.710, p = .401.

The effect is Age on Anxiety is negative (β = -0.013), meaning that anxiety decreases with age.
However, the effect of Age on Anxiety is not significant when using α=.05, t(140) = -0.842, p = .401.
`r unhide()`
