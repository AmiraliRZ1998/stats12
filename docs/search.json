[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Statistics 1 and 2",
    "section": "",
    "text": "Overview\nThis course covers the basics of statistics and data analysis. The ability to extract insights from data is an essential skill for both academic and non-academic work, and “data literacy” is increasingly important in a world where data are collected about every aspect of our lives. After completing this course, you will be able to independently analyze data, interpret and report your findings, and assess the results of analyses performed by others, such as you might find in scientific articles.\nThis GitBook contains all relevant information about this course. It is assumed that every student reads it carefully. If you have any questions, first consult this GitBook, then ask a fellow student, and only if your question is still not answered, then contact the course coordinator.\nCommunication about the course occurs through Canvas (Login with your student ID and password)."
  },
  {
    "objectID": "index.html#course-overview",
    "href": "index.html#course-overview",
    "title": "Statistics 1 and 2",
    "section": "Course overview",
    "text": "Course overview\nThe course schedule is available on Osiris. For an overview of the content, see below:\n\nWarning: package 'DT' was built under R version 4.3.1"
  },
  {
    "objectID": "index.html#literature-software",
    "href": "index.html#literature-software",
    "title": "Statistics 1 and 2",
    "section": "Literature & Software",
    "text": "Literature & Software\nYou do not need a book for this course!\nAll essential information is contained within this GitBook.\nDuring lab sessions, you work on the exercises and your portfolio using the commercial SPSS software installed on university computers.\nIf you want to use your own computer instead, you might consider trying some free alternatives to SPSS:\n\nPSPP, which is designed to be nearly identical to SPSS with all the same basic functionality: https://www.gnu.org/software/pspp/pspp.html\nJASP, which is more modern, looks nicer and is very easy to use – but looks less similar to SPSS: https://jasp-stats.org/"
  },
  {
    "objectID": "index.html#learning-goals",
    "href": "index.html#learning-goals",
    "title": "Statistics 1 and 2",
    "section": "Learning goals",
    "text": "Learning goals\nAfter taking this course, students will be able to…\nAll majors\n\ncompute and interpret commonly used descriptive statistics such as the sample mean, the median, the mode, variance and standard deviation, the standard error, and the correlation coefficient.\nrecognize different probability distributions such as the normal distribution, and make computations for these probability distributions.\nexplain the essential aspects of null-hypothesis significance testing, including sampling distributions, Type I and Type II errors, one-tailed versus two-tailed testing, and statistical power.\napply different statistical tests such as the Z-test, the one sample t-test, the one way Between Subjects Analysis of Variance test, and statistical tests related to (multiple) linear regression analysis with continuous and categorical predictors; and clarify the statistical and/or methodological assumptions that apply to the techniques that are discussed in this course.\nexplain basic concepts in regression analysis, including: linear association, least-squares estimation, explained variance, Multiple R, multiple correlation, adjusted R-square, raw and standardized regression coefficients, model-comparison tests, predicted scores, residuals and the assumptions;\nchoose the appropriate analysis technique for answering a specific research problem from the range of techniques that are covered in the course.\nuse the software package SPSS to perform several statistical data analyses and be able to correctly interpret and report the output to an informed audience (e.g., Liberal arts students, researchers from the social sciences/business and economics/cognitive neuroscience).\ndraw valid conclusions from the results of empirical data analyses given specific research questions envisaged.\nMajor Business and Economics\n\napply statistical tests in the context of multiple linear regression models with interaction terms and logistic regression models; interpret the corresponding output.\ndescribe the concepts of probabilities, odds and logits; describe the relationship between the three scales; transform one into another (formulae are provided).\nMajor Cognitive Neuroscience\n\napply statistical tests in the context of factorial ANOVA, ANCOVA and Analysis of Repeated measures; interpret the corresponding output; and calculate and interpret effect size estimates relevant for these statistical techniques (e.g., (partial) eta squared)\nMajor Social Sciences\n\napply statistical tests in the context of multiple linear regression models with interaction terms and interpret the corresponding output.\ngauge the reliability of measurements from questionnaires and identify problematic items.\nexplore the dimensionality of questionnaire data."
  },
  {
    "objectID": "index.html#attendance",
    "href": "index.html#attendance",
    "title": "Statistics 1 and 2",
    "section": "Attendance",
    "text": "Attendance\nAttendance is mandatory based on our experience that students who actively participate tend to pass the course, whereas those who do not tend to drop out or fail. All lectures and practicals ‘build’ on each other, so if you have to miss either one, absolutely make sure you have caught up with the materials before the next session."
  },
  {
    "objectID": "index.html#staff",
    "href": "index.html#staff",
    "title": "Statistics 1 and 2",
    "section": "Staff",
    "text": "Staff\nCoordinator:\ndr. Caspar J. van Lissa\nLab sessions\n(Thu) Tra Lê"
  },
  {
    "objectID": "index.html#teaching-philosophy",
    "href": "index.html#teaching-philosophy",
    "title": "Statistics 1 and 2",
    "section": "Teaching Philosophy",
    "text": "Teaching Philosophy\nThe corona crisis is challenging all of us to rethink how we teach and learn. But aside from the challenges, it also offers opportunities. In adapting this course, we kept two goals in mind: increasing the alignment between the way of teaching and the learning goals, and ensuring high-quality interaction among students and between the students and teachers while still using online communication. Based on these goals, we made the following changes:\n\nDuring the course, you will be working in learning teams to promote interaction among students and peer support\nThe two in-person exams (4 hours each) are replaced with take-home assignments: Two group assignments, and one individual assignment.\n\nWhy group assignments?\nContact with fellow students is a key aspect of the university experience. We want to stimulate you to engage with the material and with one another. Therefore, the portfolio assignments are made in groups. There are also aspects of learning in groups that can really improve your knowledge, like peer feedback. To ensure that every group member pulls their weight, the final exam tests each student’s individual comprehension of all material covered in the portfolios.\nGroups comprise 3-5 members and are assigned randomly when the course starts. However, it is allowed to switch with a consenting member of another group, or to join/merge with another small group if your group has become smaller than 3 members. There are three portfolio registration deadlines. At this point, one group member submits the definitive group composition via a Google form.\nWhy use portfolio assessment?\nPortfolio assignments are well-suited for a skills-based course like Statistics 1 & 2. They also take a lot of the pressure off because you can work at your own pace, and keep improving the work until it is good enough. We entrust you with the responsibility of making these portfolio assignments in good faith, without instrumental assistance from outside your group or plagiarism, so I kindly ask you to make good on this trust, and hand in original work to show what you’ve learned.\nUse of AI for making assignments\nThere is, in principle, nothing wrong with using AI-based tools like ChatGPT, as you will also have access to them in your working life - but be warned: when you use ChatGPT, it is your responsibility to thoroughly check its output for logical consistency and correctness. You may not yet have the level of expertise required to know when ChatGPT generates irrelevant nonsense - but the teacher who grades your work does. Consider this carefully when deciding what makes more sense: doing your work manually, making sure each step is correct - or outsourcing it to AI, and then checking its work before submitting."
  },
  {
    "objectID": "index.html#grading",
    "href": "index.html#grading",
    "title": "Statistics 1 and 2",
    "section": "Grading",
    "text": "Grading\nYour grade is based on three portfolio assignments made in groups, and one individual exam to test comprehension of the material covered in the portfolios.\nPortfolios 40% (3 x 13.3%)\nYou work on the portfolio assignments with your group, both during the lab sessions and outside of class. You hand in your group’s portfolio assignment before the set deadline, at which point it is graded. If your grade is below the passing level of 5.5, your group will have the opportunity to revise the portfolio based on teacher feedback to receive a maximum grade of 6.\nExam 60%\nTo make sure that all students are equally involved in the making of the portfolio assignments, an individual exam assesses comprehension of the material covered therein. It is a digital multiple choice exam. You may bring all course materials to the exam, including the portfolio. The exam consists of a common part and a major-specific part. Note: As per university policy, a guessing correction is applied to your grade."
  },
  {
    "objectID": "index.html#assignments",
    "href": "index.html#assignments",
    "title": "Statistics 1 and 2",
    "section": "Assignments",
    "text": "Assignments\nA description of the assignments follows below. For each assignment, every element labeled with a lower case letter is graded fail (0 points), pass (1 point), or excellent (1.5 points). Grades are summed for each assignment, and rescaled from 1-10. The final grade is the average across assignments of the rescaled grades. Note that the assignments are not full-length papers! There is a stated word limit for each section. If you can write a good report with fewer words, that’s fine. If you exceed the word limit however, your grade for that section cannot exceed a pass (1 point). The focus should be on motivating, reporting, interpreting, and discussing your analyses. For each analysis, state beforehand what your research question and hypotheses are.\nAssignment 1\nDescriptive statistics and statistical inference\n\nSelect at least three variables for further analysis, and motivate your selection based on theory, using at least one reference to explain why are you interested in the properties of the selected variables (150 words)\n\nInclude one continuous variable\nInclude one nominal variable\nInclude one ordinal variable\n\n\nDescribe the dataset (200 words + tables/figures)\n\nUse appropriate univariate descriptive statistics for all variables\nPlot data using appropriate plots\nInclude at least one frequency- or crosstable\n\n\nFor a continuous variable:\n\nSelect one or more cutoff values with clinical/societal/statistical relevance\nReport the probability of observing values that fall within/exceed those cutoff values\n\n\nFor a continuous variable:\n\nFormulate a specific null- and alternative hypothesis\nReport a one-sample t-test or Z-test for the specific null-hypothesis\nCalculate the probability of comitting a Type II error\n\n\nDiscuss your analyses (300 words)\n\nExplain your rationale for important modeling decisions\nMotivate your choice for the type of statistics and analyses\nDiscuss assumptions\nDiscuss what you have learned from it and how you might improve it\n\n\nUse APA style throughout your report\nAssignment 2\nGeneral linear model\n\nSelect at least three variables for further analysis, and using at least one reference, explain what research questions you will investigate and what hypotheses you will test (150 words)\n\nInclude one continuous outcome variable\nInclude one continuous predictor\nInclude one nominal or ordinal predictor\n\n\nConstruct a model with only the continuous predictor (200 words)\n\nReport and interpret the different sums of squares\nReport and interpret the explained variance\nConduct a separate correlation analysis. Compare the results with the regression analysis.\n\n\nConstruct a model with only the categorical predictor (200 words)\n\nReport and interpret the model results\nConduct a separate ANOVA or t-test with the same variables, whichever one is suitable. Compare the results with the regression analysis.\n\n\nConstruct a model with both the continuous and categorical predictor (200 words)\n\nReport and interpret the model results\nConduct and report a nested model test\n\n\nDiscuss your analyses (300 words)\n\nExplain your rationale for important modeling decisions\nMotivate your choice for the type of statistics and analyses\nDiscuss assumptions\nDiscuss what you have learned from it and how you might improve it\n\n\nUse APA style throughout your report\nAssignment 3 (BE)\nLogistic regression\n\nSelect at least three variables for further analysis, and using at least one reference, explain what research questions you will investigate and what hypotheses you will test (150 words)\n\nChoose a binary outcome variable\nInclude at least two predictors\n\n\nConstruct a model with only main effects (200 words)\n\nReport and interpret the results\n\n\nConstruct a model with the interaction effect (200 words)\n\nReport and interpret the model results\nConduct and report a nested model test\n\n\nThroughout your report, report confidence intervals. For at least one hypothesis, use interval testing (optionally, alongside p-value based testing).\nDiscuss your analyses (300 words)\n\nExplain your rationale for important modeling decisions\nMotivate your choice for the type of statistics and analyses\nDiscuss assumptions\nDiscuss what you have learned from it and how you might improve it\n\n\nUse APA style throughout your report\nAssignment 3 (SS)\nPsychometrics\nConstruct scale, export scores, use for interaction effects\nMajor SS: GLM VII: Interaction effects (moderation analysis) Major SS: Psychometrics I: Reliability and validity, reliability analysis Major SS: Psychometrics II: Reliability and validity, factor analysis Major SS: Psychometrics III: Reliability and validity, factor analysis\n\nSelect at least constructs for further analysis, and using at least one reference, explain what research questions you will investigate and what hypotheses you will test (150 words)\n\nAt least one of these constructs must be a scale with 3+ items\nInclude at least two predictors\n\n\nConstruct a model with only main effects (200 words)\n\nReport and interpret the results\n\n\nConstruct a model with only main effects (200 words)\n\nReport and interpret the results\n\n\nConstruct a model with the interaction effect (200 words)\n\nReport and interpret the model results\nConduct and report a nested model test\n\n\nThroughout your report, report confidence intervals. For at least one hypothesis, use interval testing (optionally, alongside p-value based testing).\nDiscuss your analyses (300 words)\n\nExplain your rationale for important modeling decisions\nMotivate your choice for the type of statistics and analyses\nDiscuss assumptions\nDiscuss what you have learned from it and how you might improve it\n\n\nUse APA style throughout your report\n\nAssignment 3 (CN)\nMajor CN: GLM IV+: Effect size, post-hoc tests and planned comparison in ANOVA Major CN: GLM IV+: Factorial designs and interaction effects in ANOVA Major CN: GLM IV+: ANCOVA (ANOVA with continuous control variables) Major CN: GLM IV+: Repeated measures ANOVA\nConduct factorial ANOVA Add continuous control variable Report effect sizes and post-hoc tests"
  },
  {
    "objectID": "index.html#credit",
    "href": "index.html#credit",
    "title": "Statistics 1 and 2",
    "section": "Credit",
    "text": "Credit\nThis book was authored by Caspar J. Van Lissa. Its code and layout are derived from Lisa DeBruine’s “booktem”:\n\nDeBruine L, Lakens D (2023). booktem: Methods Book Template. https://github.com/debruine/booktem, https://debruine.github.io/booktem/.\n\nAlso see: https://psyteachr.github.io/"
  },
  {
    "objectID": "data.html#ss-values-and-beliefs-about-individuals-and-collectives",
    "href": "data.html#ss-values-and-beliefs-about-individuals-and-collectives",
    "title": "Portfolio Data",
    "section": "SS: Values and Beliefs about Individuals and Collectives",
    "text": "SS: Values and Beliefs about Individuals and Collectives\nThe World Values Survey (WVS) is a global research project that explores people’s values and beliefs and what social and political impact these have. Among topics covered are support for democracy, tolerance of ethnic minorities, support for gender equality, the role of religion and changing levels of religiosity, the impact of globalization, attitudes toward the environment, work, family, politics, national identity, culture, diversity, insecurity, and subjective well-being. This data source is used by governments, scholars, and international organizations like the United Nations.\nExamples of research questions:\n\nWhat proportion of participants considers work to be very important in life? (Q5)\nWhat proportion of participants score more extreme than 9/10 on a left-right political ideology scale? (Q240)\nIs trust in the government significantly higher than the neutral middle of the scale (3)? (Q292O)\nDoes participants’ age predict the attitude that children should take care of their parents? (Q38 and Q262)\n\nThis synthetic dataset was inspired by Wave 7 of the World Values Survey (Haerpfer et al., 2022). All relevant information can be found here: https://www.worldvaluessurvey.org/WVSDocumentationWV7.jsp\nReference: Haerpfer, C., Inglehart, R., Moreno, A., Welzel, C., Kizilova, K., Diez-Medrano J., M. Lagos, P. Norris, E. Ponarin & B. Puranen (eds.). 2022. World Values Survey: Round Seven - Country-Pooled Datafile Version 5.0. Madrid, Spain & Vienna, Austria: JD Systems Institute & WVSA Secretariat. doi:10.14281/18241.20."
  },
  {
    "objectID": "data.html#cn-behavioral-and-neural-correlates-of-empathy-in-adolescents",
    "href": "data.html#cn-behavioral-and-neural-correlates-of-empathy-in-adolescents",
    "title": "Portfolio Data",
    "section": "CN: Behavioral and Neural Correlates of Empathy in Adolescents",
    "text": "CN: Behavioral and Neural Correlates of Empathy in Adolescents\nAdolescence is characterized by significant changes in how individuals perceive and interact with others, both cognitively and emotionally. Empathy is a crucial element in appropriately responding to the emotions and actions of others. It is often described as the capacity to understand and share the emotional experiences of others, enabling us to comprehend and anticipate their intentions. Children who possess higher levels of empathy demonstrate greater emotional regulation and engage in more prosocial behavior towards others. This experimental study presented adolescents with either positive or negative social situations, and asked them to focus either on person A or person B in those situations (in negative situations, person A was the perpetrator and person B was the victim). They then measured how many coins participants were willing to give to the focal person. Empathy was measured using a scale with three sub-dimensions of empathy (Contagion, Understanding, and Support), and brain activation in several regions of interest was measured.\nThis synthetic dataset was inspired by a study by Overgaauw and colleagues (2014).\nReference: Overgaauw, S., Güroğlu, B., Rieffe, C., & Crone, E. A. (2014). Developmental Neuroscience, 36 (3-4). Behavior and Neural Correlates of Empathy in Adolescents. https://doi.org/10.1159/000363318"
  },
  {
    "objectID": "data.html#be-sustainable-food-choices",
    "href": "data.html#be-sustainable-food-choices",
    "title": "Portfolio Data",
    "section": "BE: Sustainable Food Choices",
    "text": "BE: Sustainable Food Choices"
  },
  {
    "objectID": "introduction.html",
    "href": "introduction.html",
    "title": "1  Introduction to Statistics",
    "section": "",
    "text": "Statistics are more relevant than ever in this digital age, where data about our entire lives is readily available, and software to analyze such data has become extremely user-friendly and freely available. We live in a world where everyone wants our data, and being data literate is becoming increasingly important across industries.\nStatistics allows us to make sense of data and gain valuable insights. It helps us better understand social phenomena, predict sales and optimize marketing strategies, and even explore the relationship between brain activity and behavior. Data analysis is one of the most marketable skills taught at universities.\nBefore we delve deeper into statistics, it’s crucial to distinguish between methods and statistics. Methods refer to the procedures used in research, such as data collection, participant selection, and study design. On the other hand, statistics focuses on analyzing the data obtained from these methods.\nTwo fundamental branches of statistics covered in this course are descriptive statistics and inferential statistics. Descriptive statistics involves summarizing and describing the characteristics of a dataset, while inferential statistics allows us to make educated guesses about a larger population based on a smaller sample.\nStatistical modeling is another aspect of statistics where theories are represented mathematically. This enables us to predict important outcomes, such as sales figures, well-being, or the likelihood of neurological disorders. Statistical modeling also allows us to explore data for interesting patterns or to perform tests to answer theoretically driven research questions.\nIn scientific research, statistics can help us test theories. The process of scientific knowledge acquisition is described by the empirical cycle: We start with a theory, from which we derive testable hypotheses. A theory is an abstract system of assumptions about the relationships between constructs. A hypothesis is a concrete statement, derived from the theory, about expected quantitative relationships between measured variables. We then collect data and test the hypothesis. If the hypothesis is refuted, we re-examine the theory and possibly ammend it.\nTo lay a foundation for understanding statistics, it’s essential to be familiar with some basic concepts. First, data in the social sciences often comes in tabular format (e.g., spreadsheets), where each row represents an individual observation, and each column represents the individuals’ scores on various variables.\nA crucial distinction is the one between population and sample. The population refers to the complete set of objects of interest, such as all people in a country or all students in a class. However, due to practical limitations, we usually do not have access to the population. Instead, we draw a sample from it, which is a subset of the population. Sampling theory establishes the rationale for drawing inferences about a population based on samples. Sample statistics serve as our best estimate of population parameters. If the sample is representative, those estimates will be unbiased. Moreover, we can estimate our uncertainty about the sample statistics as estimates of population parameters. The best way to ensure a representative sample is to use random sampling, where each individual in the population has an equal chance of being included.\nThe distinction between constructs and variables is also important. Constructs are abstract features of interest within a population, like short-term memory, intelligence, or education. Variables, on the other hand, are mathematical placeholders that represent specific values associated with these constructs. Data, then, refers to the specific values of a variable.\nMeasurement level refers to the kind of information contained in a variable. The four common measurement levels are nominal, ordinal, interval, and ratio. Nominal variables have categories that differ in name only, while ordinal variables have categories with a meaningful order. Interval variables have meaningful intervals between values, which allows for meaningful comparisons between values. Finally, ratio variables have a meaningful zero-point, which additionally allows for the computation of meaningful ratios between values.\nDescriptive statistics are used to summarize and analyze data. They help us get a sense of the dataset and answer questions like the most common major among students or the average age of a group. Descriptive statistics can also be relevant in answering research questions, such as evaluating exam questions or determining if the proportion of correct answers on a multiple-choice question is greater than chance.\nIn summary, statistics is a powerful tool for analyzing and interpreting data. It helps us gain insights into various aspects of social life, make predictions, and test theories. By understanding fundamental concepts, such as measurement levels and descriptive statistics, we can start exploring and making sense of data in a meaningful way.\n\n2 Formative Test\nA formative test helps you assess your progress in the course, and helps you address any blind spots in your understanding of the material. If you get a question wrong, you will receive a hint on how to improve your understanding of the material.\nComplete the formative test ideally after you’ve seen the lecture, but before the lecture meeting in which we can discuss any topics that need more attention\n\nQuestion 1\n\nSuppose that I ask a random sample of 5 students how many pairs of shoes they have. The number of pairs are: 7, 6, 8, 6, and 8. What is the variance of these pairs of shoes?\n\n7421\n\n\nQuestion 2\n\nSix students work on a Statistics exam. They obtain the following grades: 8, 9, 5, 6, 7 and 8. The teacher calculates a certain statistic, which is equal to 7.5. Which statistic did the teacher calculate?\n\nMedianMeanStandard deviationMode\n\n\nQuestion 3\nFor which of the three scatterplots below is the correlation coefficient largest? \nA\nB\nC\n\n\n\n{width = 30%}\n\n\n{width = 30%}\n\n\n{width = 30%}\n\n\nFigure 2.1: Question 3 scatterplots\n\n\n\nSix students work on a Statistics exam. They obtain the following grades: 8, 9, 5, 6, 7 and 8. The teacher calculates a certain statistic, which is equal to 7.5. Which statistic did the teacher calculate?\n\nMedianMeanModeStandard deviation\n\n\n\n\n\nShow answers\n\nQuestion 1\nThe variance is the sum of squared distances of observations to the mean, divided by the number of observations minus one. So calculate:\n\\(S_{X}^2= \\frac{\\sum_{i=1}^nX_i}{n} = \\frac{(7 + 6 + 8 + 6 + 8)}{5} = 7\\)\nQuestion 2\nFirst rule out improbable answers; all grades are pretty close to each other, so it’s impossible for the variance to be that high. We can see what the mode (most common value) is: it’s 8. So we only choose between mean or median.\nMean: calculate \\(\\bar{X}= \\frac{\\sum_{i=1}^nX_i}{n} = \\frac{8 + 9 + 5 + 6 + 7 + 8}{6} = 7.17\\)\nMedian: order the numbers, note that there is an odd number, take the average of the two middle numbers. 5, 6, 7, 8, 8, 9 -&gt; 7.5\nQuestion 3\nCorrelation measures linear association, so eliminate option C. Option B shows a very small correlation - probably 0 or maybe .1. So the correct answer is A, which shows a moderate negative correlation."
  },
  {
    "objectID": "probability.html",
    "href": "probability.html",
    "title": "2  Probability Distributions",
    "section": "",
    "text": "Probability Distributions\nProbability refers to the likelihood or chance of an outcome occurring in a random experiment. It is defined as the proportion of times that a particular outcome is expected to occur if the experiment is repeated an infinite number of times.\nA random experiment is a process with multiple potential outcomes that could theoretically be repeated under similar conditions. For example, flipping a coin is a random experiment, and before flipping the coin, the outcome is a random experiment with a probability of getting heads or tails of 50% each. Once the coin is flipped, the outcome becomes fixed (the opposite of random), resulting in either heads or tails.\nIn a way, when you draw samples from a population and observe the values of particular variables (e.g., country of origin, height, age), you are performing random experiments. That means that, like with any random experiment, the values you are likely to observe also follow certain probability distributions. Discrete random variables have a finite or countable number of possible outcomes, such as the outcome of a coin toss. On the other hand, continuous random variables, such as the height of individuals, have an infinite number of possible outcomes.\nFor discrete (categorical) variables, we use discrete frequency and probability distributions, which summarize the observations and probabilities of each possible outcome, respectively. These distributions can be represented using frequency distributions, contingency tables, or bar charts.\nFrequency distributions summarize observed outcomes in a sample. For example, a frequency distribution can tell us the proportion of Dutch students in a class or the number of times a particular number was rolled on a die.\nContingency tables (also called crosstables) are used to describe the join frequency distribution, and possibly relationship, between two categorical variables. They show the frequencies of different combinations of values for the two variables.\nWe can use frequency distributions to estimate the probabilities of observing those outcomes in the future. To calculate probabilities from frequencies, we can use different approaches depending on the type of probability distribution we want. In general, dividing frequencies by the total number of observations (grand total) gives us probabilities. In contingency tables, marginal probability distributions are obtained by dividing the marginal totals (row sums or column sums) by the grand total, which provides us with a probability distribution for each separate variable. Conditional probability distributions are derived by dividing a specific row or column by the row- or column total (marginal total), and tells us the probabilities of one variable given a specific value of the other variable.\nIn continuous probability distributions, the possible outcomes are infinite and described by a continuous function. One common example is the normal distribution, also known as the bell curve. It is a symmetric distribution that extends from negative infinity to positive infinity, and it is characterized by two parameters: its mean (average) and standard deviation (measure of dispersion). The square of the standard deviation is called the variance.\nThe standard normal distribution, also known as the Z-distribution, is a standardized version of the normal distribution, rescaled to have a mean of 0 and a standard deviation of 1. Standardizing normal distributions allows us to calculate probabilities more easily using standard normal distribution tables or calculators. We can then convert these probabilities back to the original units if needed.\nProbability distributions can be used as models to describe/approximate the distribution of real data. Behind the scenes, we do this any time we describe the distribution of scores on a variable using its mean and standard deviation. While we often assumpe that variables are normally distributed, that assumption is not always accurate. For example, depression symptoms do not follow a normal distribution: Most people score near-zero on depression symptoms, and few people have higher scores (but these are also not normally distributed). In such cases of violations of the assumption of normality, the mean and standard deviation are not very informative. You may use other descriptive statistics, consider different probability distributions (outside the scope of this course), or discuss the limitations of the assumption of normality.\nIn conclusion, probability distributions provide a way to represent the probabilities associated with different outcomes of a random variable, whether discrete or continuous. By using probability distributions, we can report descriptive statistics, calculate probabilities, and make predictions about future observations."
  },
  {
    "objectID": "samplingdistribution.html",
    "href": "samplingdistribution.html",
    "title": "3  The Sampling Distribution",
    "section": "",
    "text": "As explained in lecture 1, a sample is an observed subset of a larger population. We typically calculate statistics based on sample data, and use these as best guesses of the values of population parameters. This process is called statistical inference. A crucial insight is that sample statistics are not perfect estimates of population parameters. The discrepancy between the sample statistic and population parameter is known as sampling error.\nWe have some theoretical insight into theoretical behavior of sample statistics. For example, we can imagine constructing a probability distribution of the values we might see for a sample statistic, such as the mean, if we were to draw very many random samples from an identical population. This theoretical distribution of means is called the sampling distribution. The central limit theorem tells us that, regardless of the shape of the distribution of the data in the population, as the sample size increases, the sampling distribution of the mean approaches a normal distribution. This is an important realization, because it means that we can use probability calculus using the normal distribution to draw inferences about population parameters based on sample statistics.\nThe standard deviation of the sampling distribution plays a central role in inferential statistics. It is so important that we give it a unique name: we call this particular standard deviation the standard error (SE). The standard error quantifies the average, or expected, amount of sampling error when we use a sample statistic to estimate the population parameter. If the standard error is small, our estimates based on the sample are likely to be accurate, whereas a large standard error indicates greater uncertainty.\nWith the help of the normal distribution, and given a particular (hypothesized or known) population mean and standard error, we can calculate how likely it is to observe specific sample means. For example, if we want to determine the probability that the mean of a random sample exceeds a certain value, we can standardize the sample mean using the formula $Z = , where M is the sample mean, \\(\\mu\\) is the known or hypothesized population mean, and SE is the standard error. By looking up the corresponding probability on the standard normal distribution table or using statistical software, we can assess the likelihood of observing a specific sample mean (or greater, or smaller).\nConfidence intervals are a way to express our uncertainty about the sample statistic as estimator of the population parameter. A confidence interval is a range of values - a window - within which we expect the true population parameter to fall with a certain level of confidence. Typically, we select a 95% confidence interval, which means that if we could repeat the sampling process many times and calculated confidence intervals each time, 95% of those intervals would contain the true population parameter. The width of the confidence interval is determined by the standard error and is proportional to the level of confidence desired. The formula for a confidence interval is often written as: \\(M \\pm Z_{95%} * SE_M\\). In practice, this comes down to approximately: \\(M \\pm 2 * SE_M\\)."
  },
  {
    "objectID": "testing.html",
    "href": "testing.html",
    "title": "4  Hypothesis Testing",
    "section": "",
    "text": "Hypothesis testing is a method of inferential statistics which allows researchers to draw conclusions about the population based on sample data. It involves formulating hypotheses, calculating test statistics, determining p-values, and drawing conclusions about the null hypothesis.\nHypothesis testing builds upon previously covered topics like sampling theory and estimation, where sample statistics are used as the best estimate of population parameters; standard errors to express the uncertainty surrounding those estimates; and probability calculus, using probability distributions - like the standard normal distribution - to compute the probability of observing certain values based on the sampling distribution.\nTo introduce the concept of hypothesis testing, let’s consider an intuitive example. Imagine your car won’t start, and you hypothesize that the battery is dead. You then perform an experiment by replacing the battery. If the car starts, you conclude that your initial hypothesis was correct - the battery was indeed dead.\nIn this thought experiment, you only need one piece of evidence. Statistical hypothesis instead rely on evidence from many observations, and use probability calculus to test hypotheses in the presence of uncertainty. Statistical tests use probability calculations to compute how probable it is to observe the sample data if the null hypothesis were true. If the resulting probability is very low, we may doubt whether the null hypothesis is indeed true.\nThe steps involved in hypothesis testing are as follows:\n\nFormulate hypotheses: This involves stating a testable proposition about population parameters.\nCalculate a test statistic: The test statistic describes how many standard errors away from the population statistic, under the null hypothesis, the sample statistic is.\nCalculate the p-value: The p-value represents the probability of observing a value at least as extreme as the sample statistic, assuming the null hypothesis is true.\nDraw a conclusion about the null hypothesis: Based on the p-value, we either reject or fail to reject the null hypothesis.\n\nHypotheses can be formulated as equality or inequality statements. Equality hypotheses state that a value, difference, or effect is equal to zero, while inequality hypotheses state that a value, difference, or effect is larger or smaller than a specific value. It’s important to keep in mind that hypothesis testing does not provide evidence for hypotheses but rather helps in casting doubt on a null hypothesis.\nIn addition to the null hypothesis, we can also specify an alternative hypothesis. The specification of the alternative hypothesis depends on a bit of philosophy of science. Fisher’s philosophy suggests using only a null hypothesis; if this null hypothesis is rejected, the “truth” must be anything other than the null hypothesis. We could thus say that, according to Fisher’s philosophy, the alternative hypothesis is the negation of the null hypothesis. If \\(H_0: \\mu = 0\\), then \\(H_a: \\mu \\neq 0\\); or, if \\(H_0: \\mu &gt; 0\\), then \\(H_a: \\mu \\leq 0\\). The alternative hypothesis is in both cases the “opposite” of the null hypothesis.\nNeyman-Pearson’s philosophy instead involves stating specific null and alternative hypotheses, with an explicit expected effect size for the alternative hypothesis. Assuming a specific expected effect size allows us to calculate the probabilities of drawing correct or incorrect conclusions.\nIn hypothesis testing, we calculate a test statistic, which measures the distance between the hypothesized population value and the sample statistic in terms of standard errors. The probability of observing a test statistic at least as extreme as the one we did observe is computed using an appropriate probability distribution. For many tests, we use either the Z-distribution or t-distribution, depending on whether we know the population standard deviation or not. This gives us a probability value (p-value), representing the probability of observing data as extreme as or more extreme than the sample data, assuming that the null hypothesis is true.\nWhen interpreting p-values, it’s crucial to understand that they give the probability of observing certain data assuming the null hypothesis is true, rather than providing the probability of the null hypothesis being true or false. The p-value is then compared to a pre-determined significance level (usually denoted as alpha) to make a decision about accepting or rejecting the null hypothesis.\nRejecting the null hypothesis indicates that the observed data is unlikely to occur if the null hypothesis were true. On the other hand, failing to reject the null hypothesis means that the observed data is not surprising or does not provide sufficient evidence to reject it.\nWhen testing hypotheses, we can make two types of errors: A Type I error refers to rejecting the null hypothesis when it is true (a false-positive conclusion), while Type II error refers to accepting the null hypothesis when it is false (failing to detect a true effect).\n\n5 Formative Test\nA formative test helps you assess your progress in the course, and helps you address any blind spots in your understanding of the material. If you get a question wrong, you will receive a hint on how to improve your understanding of the material.\nComplete the formative test ideally after you’ve seen the lecture, but before the lecture meeting in which we can discuss any topics that need more attention\n\nQuestion 1\n\nSuppose that I ask a random sample of 5 students how many pairs of shoes they have. The number of pairs are: 7, 6, 8, 6, and 8. What is the variance of these pairs of shoes?\n\n7124\n\n\nQuestion 2\n\nSix students work on a Statistics exam. They obtain the following grades: 8, 9, 5, 6, 7 and 8. The teacher calculates a certain statistic, which is equal to 7.5. Which statistic did the teacher calculate?\n\nModeMeanStandard deviationMedian\n\n\nQuestion 3\nFor which of the three scatterplots below is the correlation coefficient largest? \nA\nB\nC\n\n\n\n{width = 30%}\n\n\n{width = 30%}\n\n\n{width = 30%}\n\n\nFigure 5.1: Question 3 scatterplots\n\n\n\nSix students work on a Statistics exam. They obtain the following grades: 8, 9, 5, 6, 7 and 8. The teacher calculates a certain statistic, which is equal to 7.5. Which statistic did the teacher calculate?\n\nModeStandard deviationMedianMean\n\n\n\n\n\nShow answers\n\nQuestion 1\nThe variance is the sum of squared distances of observations to the mean, divided by the number of observations minus one. So calculate:\n\\(S_{X}^2= \\frac{\\sum_{i=1}^nX_i}{n} = \\frac{(7 + 6 + 8 + 6 + 8)}{5} = 7\\)\nQuestion 2\nFirst rule out improbable answers; all grades are pretty close to each other, so it’s impossible for the variance to be that high. We can see what the mode (most common value) is: it’s 8. So we only choose between mean or median.\nMean: calculate \\(\\bar{X}= \\frac{\\sum_{i=1}^nX_i}{n} = \\frac{8 + 9 + 5 + 6 + 7 + 8}{6} = 7.17\\)\nMedian: order the numbers, note that there is an odd number, take the average of the two middle numbers. 5, 6, 7, 8, 8, 9 -&gt; 7.5\nQuestion 3\nCorrelation measures linear association, so eliminate option C. Option B shows a very small correlation - probably 0 or maybe .1. So the correct answer is A, which shows a moderate negative correlation."
  },
  {
    "objectID": "glm1.html",
    "href": "glm1.html",
    "title": "5  GLM-I: Linear Regression",
    "section": "",
    "text": "The General Linear Model (GLM) is a family of models used to analyze the relationship between an outcome variable and one or more predictors. In this lecture, we will focus on bivariate linear regression, which describes a linear relationship between a continuous outcome variable and a continuous predictor. However, it’s important to note that the GLM encompasses other members that can handle predictors of any measurement level (continuous or categorical), multiple predictors, transformations of the outcome and predictors, and different error distributions.\nLinear regression is based on the concept of using information about other variables associated with the outcome to improve predictions. It begins with the understanding that the mean is the best predictor (expected value) when no further relevant information is available. However, if we have information about other variables, such as the number of hours studied being strongly associated with exam grades, we can use that information to enhance our predictions. This process is known as regression.\nTo visually explore associations between two variables, we often use scatterplots. Scatterplots require both variables to be at least of ordinal measurement level. By plotting the data points, we can observe whether there is a linear pattern or trend. In linear regression, we aim to find a line that represents the best possible predictions. This line, called the regression line, goes through the middle of the cloud of data points.\nThe regression line is described by the formula Y = a + bX, where “a” is the intercept (the predicted value when X equals 0) and “b” is the slope (how steeply the line increases or decreases). The predictions made using the regression line are not identical to the observed values, as there is always some prediction error. The Ordinary Least Squares method is used to obtain the line that minimizes the sum of squared prediction errors.\nIn a bivariate regression, the regression formula expands to include the individual prediction error, assuming that the errors are normally distributed around the regression line with a mean of zero. The regression model is represented as Yi = a + b * Xi + ei, where Yi is the individual’s score on the dependent variable, a is the intercept, b is the slope, Xi is the individual’s score on the independent variable, and ei is the individual prediction error.\nHypothesis tests can be conducted on the regression coefficients to determine their significance. The default null hypothesis for the intercept is that it is equal to zero, while the null hypothesis for the slope is also zero. The t-test is commonly used, with the degrees of freedom being n - p, where n is the sample size and p is the number of parameters. By testing the coefficients, we can determine the statistical significance of the relationship between the predictor and the outcome.\nWhile linear regression offers valuable insights, it is essential to consider the assumptions underlying the model. These assumptions include linearity of the relationship between the predictor and the outcome, normality of residuals (prediction errors), homoscedasticity (equal variance of residuals), and independence of observations. Violations of these assumptions can affect the validity of the model and lead to misleading results. Checking and addressing these assumptions is crucial for accurate and reliable regression analysis.\nLinear regression is a powerful tool for analyzing the relationship between variables, and a building block for many more advanced analysis techniques. It allows us to make predictions based on available information and understand the strength and significance of the relationship between a continuous predictor and continuous outcome. By considering the assumptions and conducting hypothesis tests, we can ensure the validity of our regression models and draw meaningful conclusions from the analysis."
  }
]