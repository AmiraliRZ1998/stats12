[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Statistics 1 and 2",
    "section": "",
    "text": "Overview\nThis course covers the basics of statistics and data analysis. The ability to extract insights from data is an essential skill for both academic and non-academic work, and “data literacy” is increasingly important in a world where data are collected about every aspect of our lives. After completing this course, you will be able to independently analyze data, interpret and report your findings, and assess the results of analyses performed by others, such as you might find in scientific articles.\nThis GitBook contains all relevant information about this course. It is assumed that every student reads it carefully. If you have any questions, first consult this GitBook, then ask a fellow student, and only if your question is still not answered, then contact the course coordinator."
  },
  {
    "objectID": "index.html#course-overview",
    "href": "index.html#course-overview",
    "title": "Statistics 1 and 2",
    "section": "Course overview",
    "text": "Course overview\n\nWarning: package 'DT' was built under R version 4.3.1"
  },
  {
    "objectID": "index.html#literature-software",
    "href": "index.html#literature-software",
    "title": "Statistics 1 and 2",
    "section": "Literature & Software",
    "text": "Literature & Software\nYou do not need a book for this course!\nAll essential information is contained within this GitBook.\nDuring lab sessions, you work on the exercises and your portfolio using the commercial SPSS software installed on university computers.\nIf you want to use your own computer instead, you might consider trying some free alternatives to SPSS:\n\nPSPP, which is designed to be nearly identical to SPSS with all the same basic functionality: https://www.gnu.org/software/pspp/pspp.html\nJASP, which is more modern, looks nicer and is very easy to use – but looks less similar to SPSS: https://jasp-stats.org/"
  },
  {
    "objectID": "index.html#learning-goals",
    "href": "index.html#learning-goals",
    "title": "Statistics 1 and 2",
    "section": "Learning goals",
    "text": "Learning goals\nAfter taking this course, students will be able to…\nAll majors\n\ncompute and interpret commonly used descriptive statistics such as the sample mean, the median, the mode, variance and standard deviation, the standard error, and the correlation coefficient.\nrecognize different probability distributions such as the normal distribution, and make computations for these probability distributions.\nexplain the essential aspects of null-hypothesis significance testing, including sampling distributions, Type I and Type II errors, one-tailed versus two-tailed testing, and statistical power.\napply different statistical tests such as the Z-test, the one sample t-test, the one way Between Subjects Analysis of Variance test, and statistical tests related to (multiple) linear regression analysis with continuous and categorical predictors; and clarify the statistical and/or methodological assumptions that apply to the techniques that are discussed in this course.\nexplain basic concepts in regression analysis, including: linear association, least-squares estimation, explained variance, Multiple R, multiple correlation, adjusted R-square, raw and standardized regression coefficients, model-comparison tests, predicted scores, residuals and the assumptions;\nchoose the appropriate analysis technique for answering a specific research problem from the range of techniques that are covered in the course.\nuse the software package SPSS to perform several statistical data analyses and be able to correctly interpret and report the output to an informed audience (e.g., Liberal arts students, researchers from the social sciences/business and economics/cognitive neuroscience).\ndraw valid conclusions from the results of empirical data analyses given specific research questions envisaged.\nMajor Business and Economics\n\napply statistical tests in the context of multiple linear regression models with interaction terms and logistic regression models; interpret the corresponding output.\ndescribe the concepts of probabilities, odds and logits; describe the relationship between the three scales; transform one into another (formulae are provided).\nMajor Cognitive Neuroscience\n\napply statistical tests in the context of factorial ANOVA, ANCOVA and Analysis of Repeated measures; interpret the corresponding output; and calculate and interpret effect size estimates relevant for these statistical techniques (e.g., (partial) eta squared)\nMajor Social Sciences\n\napply statistical tests in the context of multiple linear regression models with interaction terms and interpret the corresponding output.\ngauge the reliability of measurements from questionnaires and identify problematic items.\nexplore the dimensionality of questionnaire data."
  },
  {
    "objectID": "index.html#attendance",
    "href": "index.html#attendance",
    "title": "Statistics 1 and 2",
    "section": "Attendance",
    "text": "Attendance\nAttendance is mandatory based on our experience that students who actively participate tend to pass the course, whereas those who do not tend to drop out or fail. All lectures and practicals ‘build’ on each other, so if you have to miss either one, absolutely make sure you have caught up with the materials before the next session."
  },
  {
    "objectID": "index.html#staff",
    "href": "index.html#staff",
    "title": "Statistics 1 and 2",
    "section": "Staff",
    "text": "Staff\nCoordinator:\ndr. Caspar J. van Lissa\nLab sessions\n(Thu) Tra Lê"
  },
  {
    "objectID": "index.html#teaching-philosophy",
    "href": "index.html#teaching-philosophy",
    "title": "Statistics 1 and 2",
    "section": "Teaching Philosophy",
    "text": "Teaching Philosophy\nThe corona crisis is challenging all of us to rethink how we teach and learn. But aside from the challenges, it also offers opportunities. In adapting this course, we kept two goals in mind: increasing the alignment between the way of teaching and the learning goals, and ensuring high-quality interaction among students and between the students and teachers while still using online communication. Based on these goals, we made the following changes:\n\nDuring the course, you will be working in learning teams to promote interaction among students and peer support\nThe two in-person exams (4 hours each) are replaced with take-home assignments: Two group assignments, and one individual assignment.\n\nWhy group assignments?\nContact with fellow students is a key aspect of the university experience. There are also aspects of learning in groups that can really improve your knowledge, like peer feedback. The groups are made randomly when the course starts, but you can switch with a consenting member of another group before submitting your final group composition for each assignment.\nWhy use portfolio assessment?\nPortfolio assignments are well-suited for a skills-based course like Statistics 1 & 2. They also take a lot of the pressure off because you can work at your own pace, and keep improving the work until it is good enough. We entrust you with the responsibility of making these portfolio assignments in good faith, without instrumental assistance from outside your group or plagiarism, so I kindly ask you to make good on this trust, and hand in original work to show what you’ve learned.\nUse of AI for making assignments\nThere is, in principle, nothing wrong with using AI-based tools like ChatGPT, as you will also have access to them in your working life - but be warned: when you use ChatGPT, it is your responsibility to thoroughly check its output for logical consistency and correctness. You may not yet have the level of expertise required to know when ChatGPT generates irrelevant nonsense - but the teacher who grades your work does. Consider this carefully when deciding what makes more sense: doing your work manually, making sure each step is correct - or outsourcing it to AI, and then checking its work before submitting."
  },
  {
    "objectID": "index.html#grading",
    "href": "index.html#grading",
    "title": "Statistics 1 and 2",
    "section": "Grading",
    "text": "Grading\nYour grade is based on three portfolio assignments made in groups, and one individual exam to test comprehension of the material covered in the portfolios.\nPortfolios 40% (3 x 13.3%)\nYou work on the portfolio assignments with your group, both during the lab sessions and outside of class. You hand in your group’s portfolio assignment before the set deadline, at which point it is graded. If your grade is below the passing level of 5.5, your group will have the opportunity to revise the portfolio based on teacher feedback to receive a maximum grade of 6.\nExam 60%\nTo make sure that all students are equally involved in the making of the portfolio assignments, an individual exam assesses comprehension of the material covered therein. It is a digital multiple choice exam. You may bring all course materials to the exam, including the portfolio. The exam consists of a common part and a major-specific part. Note: As per university policy, a guessing correction is applied to your grade."
  },
  {
    "objectID": "index.html#assignments",
    "href": "index.html#assignments",
    "title": "Statistics 1 and 2",
    "section": "Assignments",
    "text": "Assignments\nA description of the assignments follows below. For each assignment, every element labeled with a lower case letter is graded fail (0 points), pass (1 point), or excellent (1.5 points). Grades are summed for each assignment, and rescaled from 1-10. The final grade is a weighted average across assignments of the rescaled grades (weights in %). Note that the assignments are not intended to be full-blown papers! You only get 200 words to justify your theoretical model, and 300 words to discuss the results. The focus should be on your analysis; how it relates to theory (introduction), and what you have learned from it and how you might improve it (discussion).\n\nApply the “latent variable model” to a real-life problem, where observed variables do not directly measure, but are indicators of, an unobserved social scientific construct (G, 25%)\n\nFind a suitable dataset, for example: (no word limit)\n\nData you have collected for a previous course\nOpen data, provided with a published paper\nThe “Coping with COVID-19” dataset (if you can’t find anything)\n\n\nDescribe the dataset, and introduce the theoretical latent variable model (200 words)\nEstimate the latent variable model (PCA, EFA, CFA) and conduct reliability analysis, provide relevant output in a suitable format (no word limit; as short as possible and as long as necessary to report the relevant output)\nExplain your rationale for important modeling decisions (300 words)\n\nMotivate your choice for the type of latent variable model\nDiscuss assumptions\nDiscuss other important decisions, as discussed in the course reading materials\n\n\nReport and interpret the results in APA style (no word limit; as short as possible and as long as necessary to report the relevant results)\nDiscuss the results in max 300 words\n\nDevote attention to strengths and limitations\n\n\n\n\nUse the “path model” to describe how several variables are causally related to one another (G, 25%)\n\nFind a suitable dataset, for example: (no word limit)\n\nData you have collected and analyzed for a previous course\nOpen data, provided with a published paper\nThe “Coping with COVID-19” dataset (if you can’t find anything)\n\n\nDescribe the dataset, and introduce the theoretical path model (200 words)\nConduct a SEM path model to answer the theoretical questions (no word limit; as short as possible and as long as necessary to report the relevant output)\n\nThis can be a re-analysis of a question that had been tested using regression, ANOVA, or t-test analysis in the original paper\n\n\nExplain your rationale for important modeling decisions (300 words)\n\nFit between theory and model\nModel assumptions\nDifference/similarity between the path model and the (original) regression, ANOVA, or t-test analysis\nWhy you use standardized or unstandardized coefficients\n\n\nReport and interpret the results in APA style (no word limit; as short as possible and as long as necessary to report the relevant results)\n\nInclude measures of explained variance for the dependent variables.\n\n\nDiscuss the results (max 300 words)\n\nDevote attention to strengths and limitations\n\n\n\n\nIndependently analyze data using the free, open-source statistical software R (I, 50%)\n\nFind a suitable dataset, for example: (no word limit)\n\nData you have collected for a previous course\nOpen data, provided with a published paper\nThe “Coping with COVID-19” dataset (if you can’t find anything)\n\n\nDescribe the dataset, and introduce a theory involving at least 3 variables that can be tested using these data (300 words)\nTranslate the theory to lavaan syntax and estimate the model (could be multiple models if you think it’s necessary; no word limit)\nUse at least all of the following: (no word limit)\n\nOne latent variable\nModeration (continuous or multi-group)\nMediation\n\n\nExplain your rationale for important modeling decisions (300 words)\n\nFit between theory and model\nModel assumptions\nDifference/similarity between the path model and the (original) regression, ANOVA, or t-test analysis\nWhy you use standardized or unstandardized coefficients\n\n\nReport and interpret your results in APA style (no word limit; as short as possible and as long as necessary to report the relevant results)\nDiscuss your results in maximum 500 words"
  },
  {
    "objectID": "index.html#credit",
    "href": "index.html#credit",
    "title": "Statistics 1 and 2",
    "section": "Credit",
    "text": "Credit\nThis book is based on Lisa DeBruine’s “booktem”:\n\nDeBruine L, Lakens D (2023). booktem: Methods Book Template. https://github.com/debruine/booktem, https://debruine.github.io/booktem/.\n\nAlso see: https://psyteachr.github.io/"
  },
  {
    "objectID": "introduction.html",
    "href": "introduction.html",
    "title": "1  Introduction to Statistics",
    "section": "",
    "text": "Statistics are more relevant than ever in this digital age, where data about our entire lives is readily available, and software to analyze such data has become extremely user-friendly and freely available. We live in a world where everyone wants our data, and being data literate is becoming increasingly important across industries.\nStatistics allows us to make sense of data and gain valuable insights. It helps us better understand social phenomena, predict sales and optimize marketing strategies, and even explore the relationship between brain activity and behavior. Data analysis is one of the most marketable skills taught at universities.\nBefore we delve deeper into statistics, it’s crucial to distinguish between methods and statistics. Methods refer to the procedures used in research, such as data collection, participant selection, and study design. On the other hand, statistics focuses on analyzing the data obtained from these methods.\nTwo fundamental branches of statistics covered in this course are descriptive statistics and inferential statistics. Descriptive statistics involves summarizing and describing the characteristics of a dataset, while inferential statistics allows us to make educated guesses about a larger population based on a smaller sample.\nStatistical modeling is another aspect of statistics where theories are represented mathematically. This enables us to predict important outcomes, such as sales figures, well-being, or the likelihood of neurological disorders. Statistical modeling also allows us to explore data for interesting patterns or to perform tests to answer theoretically driven research questions.\nIn scientific research, statistics can help us test theories. The process of scientific knowledge acquisition is described by the empirical cycle: We start with a theory, from which we derive testable hypotheses. A theory is an abstract system of assumptions about the relationships between constructs. A hypothesis is a concrete statement, derived from the theory, about expected quantitative relationships between measured variables. We then collect data and test the hypothesis. If the hypothesis is refuted, we re-examine the theory and possibly ammend it.\nTo lay a foundation for understanding statistics, it’s essential to be familiar with some basic concepts. First, data in the social sciences often comes in tabular format (e.g., spreadsheets), where each row represents an individual observation, and each column represents the individuals’ scores on various variables.\nA crucial distinction is the one between population and sample. The population refers to the complete set of objects of interest, such as all people in a country or all students in a class. However, due to practical limitations, we usually do not have access to the population. Instead, we draw a sample from it, which is a subset of the population. Sampling theory establishes the rationale for drawing inferences about a population based on samples. Sample statistics serve as our best estimate of population parameters. If the sample is representative, those estimates will be unbiased. Moreover, we can estimate our uncertainty about the sample statistics as estimates of population parameters. The best way to ensure a representative sample is to use random sampling, where each individual in the population has an equal chance of being included.\nThe distinction between constructs and variables is also important. Constructs are abstract features of interest within a population, like short-term memory, intelligence, or education. Variables, on the other hand, are mathematical placeholders that represent specific values associated with these constructs. Data, then, refers to the specific values of a variable.\nMeasurement level refers to the kind of information contained in a variable. The four common measurement levels are nominal, ordinal, interval, and ratio. Nominal variables have categories that differ in name only, while ordinal variables have categories with a meaningful order. Interval variables have meaningful intervals between values, which allows for meaningful comparisons between values. Finally, ratio variables have a meaningful zero-point, which additionally allows for the computation of meaningful ratios between values.\nDescriptive statistics are used to summarize and analyze data. They help us get a sense of the dataset and answer questions like the most common major among students or the average age of a group. Descriptive statistics can also be relevant in answering research questions, such as evaluating exam questions or determining if the proportion of correct answers on a multiple-choice question is greater than chance.\nIn summary, statistics is a powerful tool for analyzing and interpreting data. It helps us gain insights into various aspects of social life, make predictions, and test theories. By understanding fundamental concepts, such as measurement levels and descriptive statistics, we can start exploring and making sense of data in a meaningful way.\n\n2 Formative Test\nA formative test helps you assess your progress in the course, and helps you address any blind spots in your understanding of the material. If you get a question wrong, you will receive a hint on how to improve your understanding of the material.\nComplete the formative test ideally after you’ve seen the lecture, but before the lecture meeting in which we can discuss any topics that need more attention\n\nQuestion 1\n\nSuppose that I ask a random sample of 5 students how many pairs of shoes they have. The number of pairs are: 7, 6, 8, 6, and 8. What is the variance of these pairs of shoes?\n\n7421\n\n\nQuestion 2\n\nSix students work on a Statistics exam. They obtain the following grades: 8, 9, 5, 6, 7 and 8. The teacher calculates a certain statistic, which is equal to 7.5. Which statistic did the teacher calculate?\n\nMedianMeanStandard deviationMode\n\n\nQuestion 3\nFor which of the three scatterplots below is the correlation coefficient largest? \nA\nB\nC\n\n\n\n{width = 30%}\n\n\n{width = 30%}\n\n\n{width = 30%}\n\n\nFigure 2.1: Question 3 scatterplots\n\n\n\nSix students work on a Statistics exam. They obtain the following grades: 8, 9, 5, 6, 7 and 8. The teacher calculates a certain statistic, which is equal to 7.5. Which statistic did the teacher calculate?\n\nMedianMeanModeStandard deviation\n\n\n\n\n\nShow answers\n\nQuestion 1\nThe variance is the sum of squared distances of observations to the mean, divided by the number of observations minus one. So calculate:\n\\(S_{X}^2= \\frac{\\sum_{i=1}^nX_i}{n} = \\frac{(7 + 6 + 8 + 6 + 8)}{5} = 7\\)\nQuestion 2\nFirst rule out improbable answers; all grades are pretty close to each other, so it’s impossible for the variance to be that high. We can see what the mode (most common value) is: it’s 8. So we only choose between mean or median.\nMean: calculate \\(\\bar{X}= \\frac{\\sum_{i=1}^nX_i}{n} = \\frac{8 + 9 + 5 + 6 + 7 + 8}{6} = 7.17\\)\nMedian: order the numbers, note that there is an odd number, take the average of the two middle numbers. 5, 6, 7, 8, 8, 9 -&gt; 7.5\nQuestion 3\nCorrelation measures linear association, so eliminate option C. Option B shows a very small correlation - probably 0 or maybe .1. So the correct answer is A, which shows a moderate negative correlation."
  },
  {
    "objectID": "probability.html",
    "href": "probability.html",
    "title": "2  Probability Distributions",
    "section": "",
    "text": "Probability Distributions\nProbability refers to the likelihood or chance of an outcome occurring in a random experiment. It is defined as the proportion of times that a particular outcome is expected to occur if the experiment is repeated an infinite number of times.\nA random experiment is a process with multiple potential outcomes that could theoretically be repeated under similar conditions. For example, flipping a coin is a random experiment, and before flipping the coin, the outcome is a random experiment with a probability of getting heads or tails of 50% each. Once the coin is flipped, the outcome becomes fixed (the opposite of random), resulting in either heads or tails.\nIn a way, when you draw samples from a population and observe the values of particular variables (e.g., country of origin, height, age), you are performing random experiments. That means that, like with any random experiment, the values you are likely to observe also follow certain probability distributions. Discrete random variables have a finite or countable number of possible outcomes, such as the outcome of a coin toss. On the other hand, continuous random variables, such as the height of individuals, have an infinite number of possible outcomes.\nFor discrete (categorical) variables, we use discrete frequency and probability distributions, which summarize the observations and probabilities of each possible outcome, respectively. These distributions can be represented using frequency distributions, contingency tables, or bar charts.\nFrequency distributions summarize observed outcomes in a sample. For example, a frequency distribution can tell us the proportion of Dutch students in a class or the number of times a particular number was rolled on a die.\nContingency tables (also called crosstables) are used to describe the join frequency distribution, and possibly relationship, between two categorical variables. They show the frequencies of different combinations of values for the two variables.\nWe can use frequency distributions to estimate the probabilities of observing those outcomes in the future. To calculate probabilities from frequencies, we can use different approaches depending on the type of probability distribution we want. In general, dividing frequencies by the total number of observations (grand total) gives us probabilities. In contingency tables, marginal probability distributions are obtained by dividing the marginal totals (row sums or column sums) by the grand total, which provides us with a probability distribution for each separate variable. Conditional probability distributions are derived by dividing a specific row or column by the row- or column total (marginal total), and tells us the probabilities of one variable given a specific value of the other variable.\nIn continuous probability distributions, the possible outcomes are infinite and described by a continuous function. One common example is the normal distribution, also known as the bell curve. It is a symmetric distribution that extends from negative infinity to positive infinity, and it is characterized by two parameters: its mean (average) and standard deviation (measure of dispersion). The square of the standard deviation is called the variance.\nThe standard normal distribution, also known as the Z-distribution, is a standardized version of the normal distribution, rescaled to have a mean of 0 and a standard deviation of 1. Standardizing normal distributions allows us to calculate probabilities more easily using standard normal distribution tables or calculators. We can then convert these probabilities back to the original units if needed.\nProbability distributions can be used as models to describe/approximate the distribution of real data. Behind the scenes, we do this any time we describe the distribution of scores on a variable using its mean and standard deviation. While we often assumpe that variables are normally distributed, that assumption is not always accurate. For example, depression symptoms do not follow a normal distribution: Most people score near-zero on depression symptoms, and few people have higher scores (but these are also not normally distributed). In such cases of violations of the assumption of normality, the mean and standard deviation are not very informative. You may use other descriptive statistics, consider different probability distributions (outside the scope of this course), or discuss the limitations of the assumption of normality.\nIn conclusion, probability distributions provide a way to represent the probabilities associated with different outcomes of a random variable, whether discrete or continuous. By using probability distributions, we can report descriptive statistics, calculate probabilities, and make predictions about future observations."
  },
  {
    "objectID": "samplingdistribution.html",
    "href": "samplingdistribution.html",
    "title": "3  The Sampling Distribution",
    "section": "",
    "text": "As explained in lecture 1, a sample is an observed subset of a larger population. We typically calculate statistics based on sample data, and use these as best guesses of the values of population parameters. This process is called statistical inference. A crucial insight is that sample statistics are not perfect estimates of population parameters. The discrepancy between the sample statistic and population parameter is known as sampling error.\nWe have some theoretical insight into theoretical behavior of sample statistics. For example, we can imagine constructing a probability distribution of the values we might see for a sample statistic, such as the mean, if we were to draw very many random samples from an identical population. This theoretical distribution of means is called the sampling distribution. The central limit theorem tells us that, regardless of the shape of the distribution of the data in the population, as the sample size increases, the sampling distribution of the mean approaches a normal distribution. This is an important realization, because it means that we can use probability calculus using the normal distribution to draw inferences about population parameters based on sample statistics.\nThe standard deviation of the sampling distribution plays a central role in inferential statistics. It is so important that we give it a unique name: we call this particular standard deviation the standard error (SE). The standard error quantifies the average, or expected, amount of sampling error when we use a sample statistic to estimate the population parameter. If the standard error is small, our estimates based on the sample are likely to be accurate, whereas a large standard error indicates greater uncertainty.\nWith the help of the normal distribution, and given a particular (hypothesized or known) population mean and standard error, we can calculate how likely it is to observe specific sample means. For example, if we want to determine the probability that the mean of a random sample exceeds a certain value, we can standardize the sample mean using the formula $Z = , where M is the sample mean, \\(\\mu\\) is the known or hypothesized population mean, and SE is the standard error. By looking up the corresponding probability on the standard normal distribution table or using statistical software, we can assess the likelihood of observing a specific sample mean (or greater, or smaller).\nConfidence intervals are a way to express our uncertainty about the sample statistic as estimator of the population parameter. A confidence interval is a range of values - a window - within which we expect the true population parameter to fall with a certain level of confidence. Typically, we select a 95% confidence interval, which means that if we could repeat the sampling process many times and calculated confidence intervals each time, 95% of those intervals would contain the true population parameter. The width of the confidence interval is determined by the standard error and is proportional to the level of confidence desired. The formula for a confidence interval is often written as: \\(M \\pm Z_{95%} * SE_M\\). In practice, this comes down to approximately: \\(M \\pm 2 * SE_M\\)."
  },
  {
    "objectID": "glm1.html",
    "href": "glm1.html",
    "title": "4  GLM-I: Linear Regression",
    "section": "",
    "text": "The General Linear Model (GLM) is a family of models used to analyze the relationship between an outcome variable and one or more predictors. In this lecture, we will focus on bivariate linear regression, which describes a linear relationship between a continuous outcome variable and a continuous predictor. However, it’s important to note that the GLM encompasses other members that can handle predictors of any measurement level (continuous or categorical), multiple predictors, transformations of the outcome and predictors, and different error distributions.\nLinear regression is based on the concept of using information about other variables associated with the outcome to improve predictions. It begins with the understanding that the mean is the best predictor (expected value) when no further relevant information is available. However, if we have information about other variables, such as the number of hours studied being strongly associated with exam grades, we can use that information to enhance our predictions. This process is known as regression.\nTo visually explore associations between two variables, we often use scatterplots. Scatterplots require both variables to be at least of ordinal measurement level. By plotting the data points, we can observe whether there is a linear pattern or trend. In linear regression, we aim to find a line that represents the best possible predictions. This line, called the regression line, goes through the middle of the cloud of data points.\nThe regression line is described by the formula Y = a + bX, where “a” is the intercept (the predicted value when X equals 0) and “b” is the slope (how steeply the line increases or decreases). The predictions made using the regression line are not identical to the observed values, as there is always some prediction error. The Ordinary Least Squares method is used to obtain the line that minimizes the sum of squared prediction errors.\nIn a bivariate regression, the regression formula expands to include the individual prediction error, assuming that the errors are normally distributed around the regression line with a mean of zero. The regression model is represented as Yi = a + b * Xi + ei, where Yi is the individual’s score on the dependent variable, a is the intercept, b is the slope, Xi is the individual’s score on the independent variable, and ei is the individual prediction error.\nHypothesis tests can be conducted on the regression coefficients to determine their significance. The default null hypothesis for the intercept is that it is equal to zero, while the null hypothesis for the slope is also zero. The t-test is commonly used, with the degrees of freedom being n - p, where n is the sample size and p is the number of parameters. By testing the coefficients, we can determine the statistical significance of the relationship between the predictor and the outcome.\nWhile linear regression offers valuable insights, it is essential to consider the assumptions underlying the model. These assumptions include linearity of the relationship between the predictor and the outcome, normality of residuals (prediction errors), homoscedasticity (equal variance of residuals), and independence of observations. Violations of these assumptions can affect the validity of the model and lead to misleading results. Checking and addressing these assumptions is crucial for accurate and reliable regression analysis.\nLinear regression is a powerful tool for analyzing the relationship between variables, and a building block for many more advanced analysis techniques. It allows us to make predictions based on available information and understand the strength and significance of the relationship between a continuous predictor and continuous outcome. By considering the assumptions and conducting hypothesis tests, we can ensure the validity of our regression models and draw meaningful conclusions from the analysis."
  }
]