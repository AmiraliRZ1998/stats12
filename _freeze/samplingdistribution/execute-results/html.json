{
  "hash": "5d1eb3121ca4380db29bdd58bb926df2",
  "result": {
    "markdown": "# The Sampling Distribution {#sec-sampdist}\n\n\n\n\n\nAs explained in lecture 1, a sample is an observed subset of a larger population.\nWe typically calculate statistics based on sample data, and use these as best guesses of the values of population parameters.\nThis process is called statistical inference. \nA crucial insight is that sample statistics are not perfect estimates of population parameters. The discrepancy between the sample statistic and population parameter is known as sampling error.\n\nWe have some theoretical insight into theoretical behavior of sample statistics.\nFor example, we can imagine constructing a probability distribution of the values we might see for a sample statistic, such as the mean, if we were to draw very many random samples from an identical population.\nThis theoretical distribution of means is called the sampling distribution.\nThe central limit theorem tells us that, regardless of the shape of the distribution of the data in the population, as the sample size increases, the sampling distribution of the mean approaches a normal distribution.\nThis is an important realization, because it means that we can use probability calculus using the normal distribution to draw inferences about population parameters based on sample statistics.\n\nThe standard deviation of the sampling distribution plays a central role in inferential statistics.\nIt is so important that we give it a unique name: we call this particular standard deviation the *standard error* (SE).\nThe standard error quantifies the average, or expected, amount of sampling error when we use a sample statistic to estimate the population parameter.\nIf the standard error is small, our estimates based on the sample are likely to be accurate, whereas a large standard error indicates greater uncertainty.\n\nWith the help of the normal distribution,\nand given a particular (hypothesized or known) population mean and standard error,\nwe can calculate how likely it is to observe specific sample means.\nFor example, if we want to determine the probability that the mean of a random sample exceeds a certain value, we can standardize the sample mean using the formula $Z = \\frac{M - \\mu}{SE_M}, where M is the sample mean, $\\mu$ is the known or hypothesized population mean,\nand SE is the standard error.\nBy looking up the corresponding probability on the standard normal distribution table or using statistical software, we can assess the likelihood of observing a specific sample mean (or greater, or smaller).\n\nConfidence intervals are a way to express our uncertainty about the sample statistic as estimator of the population parameter.\nA confidence interval is a range of values - a window - within which we expect the true population parameter to fall with a certain level of confidence.\nTypically, we select a 95% confidence interval, which means that if we could repeat the sampling process many times and calculated confidence intervals each time, 95% of those intervals would contain the true population parameter.\nThe width of the confidence interval is determined by the standard error and is proportional to the level of confidence desired.\nThe formula for a confidence interval is often written as: $M \\pm Z_{95%} * SE_M$.\nIn practice, this comes down to approximately: $M \\pm 2 * SE_M$.\n",
    "supporting": [
      "samplingdistribution_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}