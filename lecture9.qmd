---
title: "Lecture 9 - GLM V\nMultiple Regression"
author:   "Caspar J. van Lissa"
date:     "`r format(Sys.Date(), '%d %b %Y')`"
format: revealjs
server: shiny
---

```{r}
library(kableExtra)
library(tidySEM)
options(knitr.kable.NA = '')

set.seed(5793)
data <- data.frame(
  sex_dich = ordered(sample(c("Man", "Woman"), size = 100, replace = TRUE), levels = c("Woman", "Man"))
)
data$shoesize <- rnorm(100, mean = c(39, 43)[as.integer(data$sex_dich)], sd = 2)
```


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(ggplot2)
```

# Recap

## Two+ categories {.smaller}

Remember: we can use bivariate linear regression to model two categories

$\hat{Y}_i = a + b*X_i$

* a: Mean of group 1
* b: Mean difference between groups 1 and 2

For **three+** categories, we can **expand** the model:

$\hat{Y}_i = a + b_1*X_{1i}+ b_2*X_{2i}$

* a: Mean of group 1
* $b_1$: Mean difference between groups 1 and 2
* $b_2$: Mean difference between groups 1 and 3

## Multiple regression

Last week's model with two dummies was already an example of $\color{blue}{\text{multiple regression}}$.

**Multiple regressie:** Regression with more than one predictor.

It answers the question: What is the unique effect of one predictor, controlling for the effect of all other predictors?

## Multiple regression

$\hat{Y}_i = a + b_1*X_{1i}+ b_2*X_{2i}$

Last week, $X_1$ and $X_2$ were dummies (only 0 and 1 values)

You can simply replace them with continuous predictors!

You can expand the model with as many predictors as you like:

$\hat{Y}_i = a + b_1*X_{1i}+ b_2*X_{2i} + b_3*X_{3i} + \dots + b_K*X_{Ki}$

## Parameter interpretation

$\hat{Y}_i = a + b_1*X_{1i}+ b_2*X_{2i}$

$a$ is the $\color{blue}{\text{intercept}}$

* Expected value when **all** predictors are equal to 0
* When using dummies, this is the mean value of the reference category
* When using continuous predictors, this is the expected value for someone who scores 0 on all predictors

$b_1$ and $b_2$ are $\color{blue}{\text{slopes}}$

* There's one $b$ for each $X$
* $b$ tells us how many points Y increases if X goes up by 1, while keeping **all other** X-values equal

## Unique effects

**Aim:** predict dependent variable Y from multiple predictors $X_1, X_2, \ldots,X_k$ with a linear model:

$y_i = b_0 + b_1 * x_1 + b_2 * x_2 + \ldots + b_k * x_k + \epsilon_i$

This will give you the **unique/partial effect** of each predictor, while keeping all other variables constant

![](images/venn_reading.png)

## When to use?

* When theory implies multiple causes
* To make better predictions using all available predictors
* To compare relative importance of different predictors
* To improve causal inference

# Standardized regression coefficients

## Standardizing regression coefficients

**Problem:** We want to know how important different predictors are

**Problem:** We want to compare the effect of the same variable across two studies

Solution: Standardize the regression coefficient to make them ~comparable (but there are limitations)

## What is standardized regression coefficient

It's just the regression coefficient you would get IF you carried out the analysis after standardizing the X and Y variables

Instead of X and Y, we use Z-scores:

$Z_x = (X - \bar{X}) / SD_x$
$Z_y = (Y - \bar{Y}) / SD_y$

Z-scores: mean = 0,  SD = 1

Z-scores lose the original units of a variable. The new unit is the SD: a Z-score of 1.3 means "1.3 standard deviations above the mean"

## Interpretation

**Unstandardized**

A one-point increase in X is associated with a $b$-point increase in Y

**Standardized**

A one-SD increase in X is associated with a $\beta$ SD increase in Y


## When to use (un)standardized coefficients?

**Unstandardized**

* If the units are meaningful/important (e.g., years, euros, centimeters, number of questions correct)
* If there are (clinical) cut-off scores

**Standardized**

* When units are not meaningful (e.g., depression, need to belong, job satisfaction, Likert scales).
* If you want to compare effect sizes / variable importance

In this course, we will mostly focus on the unstandardized regression coefficient b

# Causality

## Causality

* Often, we want to find causal relationships: X -> Y
    + Treatment, Policy decisions, Investments
* Causality can only be established using experiments, or assumed based on theory
* If our theory implies alternative explanations, we can account for these using multiple regression
    + Our theory could be wrong. In this case, our analysis can give misleading results

## Types of multivariate relationships

* Spurious association between X and Y (there is common “cause” to both)
* Mediation effect X -> M -> Y (chain relationship)
* Multiple causes (e.g., job performance = motivation + education)
* Interaction: (the effect of X on Y depends on the level of a third variable, M)

## Types of multivariate relationships

```{r, out.width="90%"}
knitr::include_graphics("./materials/types_relationships.png")
```

## Types, continued

```{r, out.width="90%"}
knitr::include_graphics("./materials/confounders.png")
```

[Further reading about causal inference for the curious](https://theoreticalecology.wordpress.com/2019/04/14/mediators-confounders-colliders-a-crash-course-in-causal-inference/)

## Confounders

* A confounding variable causes BOTH X and Y
* This inflates the observed relationship between X and Y
* **Assuming that your model is correct**, controlling for confounders improves causal inference 
* Common suspected confounders are gender, age, education
    + Controlling for a variable that is not causally related to the outcome can bias your results, so don't put EVERYTHING in the model without good reason

## Confounder example

```{r}
knitr::include_graphics("./materials/control_height_vocab.png")
```

## Confounder example

```{r}
knitr::include_graphics("./materials/control_height_vocab2.png")
```

## Confounder example 2

```{r}
knitr::include_graphics("./materials/control_stress.png")
```

## Confounder example 2

```{r}
knitr::include_graphics("./materials/control_stress2.png")
```

## More next week...

... then we will carry out multiple regression analysis with two predictors!

Reference material for next week:

* Book 2(!): Chapter 4: 4.6 and 4.7.3
*	Chapter 5: 5.1, 5.8 and 5.9
